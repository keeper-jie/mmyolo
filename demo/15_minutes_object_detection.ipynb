{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x7seefPduh36"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <img width=\"600\" src=\"https://github.com/open-mmlab/mmyolo/raw/main/resources/mmyolo-logo.png\"/>\n",
        "  <div>&nbsp;</div>\n",
        "  <div align=\"center\">\n",
        "    <b><font size=\"5\">OpenMMLab website</font></b>\n",
        "    <sup>\n",
        "      <a href=\"https://openmmlab.com\">\n",
        "        <i><font size=\"4\">HOT</font></i>\n",
        "      </a>\n",
        "    </sup>\n",
        "    &nbsp;&nbsp;&nbsp;&nbsp;\n",
        "    <b><font size=\"5\">OpenMMLab platform</font></b>\n",
        "    <sup>\n",
        "      <a href=\"https://platform.openmmlab.com\">\n",
        "        <i><font size=\"4\">TRY IT OUT</font></i>\n",
        "      </a>\n",
        "    </sup>\n",
        "  </div>\n",
        "  <div>&nbsp;</div>\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/open-mmlab/mmyolo/blob/dev/demo/15_minutes_object_detection.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "\n",
        "[![PyPI](https://img.shields.io/pypi/v/mmyolo)](https://pypi.org/project/mmyolo)\n",
        "[![docs](https://img.shields.io/badge/docs-latest-blue)](https://mmyolo.readthedocs.io/en/latest/)\n",
        "[![deploy](https://github.com/open-mmlab/mmyolo/workflows/deploy/badge.svg)](https://github.com/open-mmlab/mmyolo/actions)\n",
        "[![codecov](https://codecov.io/gh/open-mmlab/mmyolo/branch/main/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmyolo)\n",
        "[![license](https://img.shields.io/github/license/open-mmlab/mmyolo.svg)](https://github.com/open-mmlab/mmyolo/blob/main/LICENSE)\n",
        "[![open issues](https://isitmaintained.com/badge/open/open-mmlab/mmyolo.svg)](https://github.com/open-mmlab/mmyolo/issues)\n",
        "[![issue resolution](https://isitmaintained.com/badge/resolution/open-mmlab/mmyolo.svg)](https://github.com/open-mmlab/mmyolo/issues)\n",
        "\n",
        "[üìòDocumentation](https://mmyolo.readthedocs.io/en/latest/) |\n",
        "[üõ†Ô∏èInstallation](https://mmyolo.readthedocs.io/en/latest/get_started/installation.html) |\n",
        "[üëÄModel Zoo](https://mmyolo.readthedocs.io/en/latest/model_zoo.html) |\n",
        "[üÜïUpdate News](https://mmyolo.readthedocs.io/en/latest/notes/changelog.html) |\n",
        "[ü§îReporting Issues](https://github.com/open-mmlab/mmyolo/issues/new/choose)\n",
        "\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "  <a href=\"https://openmmlab.medium.com/\" style=\"text-decoration:none;\">\n",
        "    <img src=\"https://user-images.githubusercontent.com/25839884/219255827-67c1a27f-f8c5-46a9-811d-5e57448c61d1.png\" width=\"3%\" alt=\"\" /></a>\n",
        "  <img src=\"https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png\" width=\"3%\" alt=\"\" />\n",
        "  <a href=\"https://discord.com/channels/1037617289144569886/1046608014234370059\" style=\"text-decoration:none;\">\n",
        "    <img src=\"https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png\" width=\"3%\" alt=\"\" /></a>\n",
        "  <img src=\"https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png\" width=\"3%\" alt=\"\" />\n",
        "  <a href=\"https://twitter.com/OpenMMLab\" style=\"text-decoration:none;\">\n",
        "    <img src=\"https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png\" width=\"3%\" alt=\"\" /></a>\n",
        "  <img src=\"https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png\" width=\"3%\" alt=\"\" />\n",
        "  <a href=\"https://www.youtube.com/openmmlab\" style=\"text-decoration:none;\">\n",
        "    <img src=\"https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png\" width=\"3%\" alt=\"\" /></a>\n",
        "  <img src=\"https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png\" width=\"3%\" alt=\"\" />\n",
        "  <a href=\"https://space.bilibili.com/1293512903\" style=\"text-decoration:none;\">\n",
        "    <img src=\"https://user-images.githubusercontent.com/25839884/219026751-d7d14cce-a7c9-4e82-9942-8375fca65b99.png\" width=\"3%\" alt=\"\" /></a>\n",
        "  <img src=\"https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png\" width=\"3%\" alt=\"\" />\n",
        "  <a href=\"https://www.zhihu.com/people/openmmlab\" style=\"text-decoration:none;\">\n",
        "    <img src=\"https://user-images.githubusercontent.com/25839884/219026120-ba71e48b-6e94-4bd4-b4e9-b7d175b5e362.png\" width=\"3%\" alt=\"\" /></a>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V6W8P5XEJGoc"
      },
      "source": [
        "# 15 minutes to get started with MMYOLO object detection\n",
        "\n",
        "Object detection task refers to that given a picture, the network predicts all the categories of objects included in the picture and the corresponding boundary boxes\n",
        "\n",
        "<div align=center>\n",
        "<img src=\"https://user-images.githubusercontent.com/17425982/220232979-fffa480b-9ae6-4601-8af6-4116265dc650.png\" alt=\"object detection\" width=\"800\"/>\n",
        "</div>\n",
        "\n",
        "Take the small dataset of cat as an example, you can easily learn MMYOLO object detection in 15 minutes. The whole process consists of the following steps:\n",
        "\n",
        "- [Installation](#installation)\n",
        "- [Dataset](#dataset)\n",
        "- [Config](#config)\n",
        "- [Training](#training)\n",
        "- [Testing](#testing)\n",
        "- [EasyDeploy](#easydeploy-deployment)\n",
        "\n",
        "In this tutorial, we take YOLOv5-s as an example. For the rest of the YOLO series algorithms, please see the corresponding algorithm configuration folder."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae5SqsA7wYGQ"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Assuming you've already installed Conda in advance, then install PyTorch using the following commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVLRaEIzwW-6",
        "outputId": "901b5db6-b1d7-4830-e746-485ee76d6648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.7.10\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n",
            "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "Copyright (C) 2017 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------------------\n",
        "# If you are using colab, you can skip this cell for PyTorch is pre-installed on the colab.\n",
        "# -----------------------------------------------------------------------------------------\n",
        "!python -V\n",
        "# Check nvcc version\n",
        "!nvcc -V\n",
        "# Check GCC version\n",
        "!gcc --version\n",
        "# Create a new Conda environment\n",
        "# %conda create -n mmyolo python=3.8 -y\n",
        "# %conda activate mmyolo\n",
        "# If you have GPU\n",
        "# %conda install pytorch torchvision -c pytorch\n",
        "# If you only have CPU\n",
        "# %conda install pytorch torchvision cpuonly -c pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.9.0\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# Check PyTorch version\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install MMYOLO and dependency libraries using the following commands.\n",
        "For details about how to configure the environment, see [Installation and verification](https://mmyolo.readthedocs.io/en/latest/get_started/installation.html).\n",
        "```{note}\n",
        "Note: Since this repo uses OpenMMLab 2.0, it is better to create a new conda virtual environment to prevent conflicts with the repo installed in OpenMMLab 1.0.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qATUuntwmfD",
        "outputId": "24be577b-efce-46f2-8b2f-a65d02824467"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/open-mmlab/mmyolo.git\n",
        "# %cd mmyolo\n",
        "# %pip install -U openmim\n",
        "# !mim install -r requirements/mminstall.txt\n",
        "# Install albumentations\n",
        "# !mim install -r requirements/albu.txt\n",
        "# Install MMYOLO\n",
        "# !mim install -v -e .\n",
        "# \"-v\" means verbose, or more output\n",
        "# \"-e\" means installing a project in editable mode,\n",
        "# thus any local modifications made to the code will take effect without reinstallation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "\n",
        "The Cat dataset is a single-category dataset consisting of 144 pictures (the original pictures are provided by @RangeKing, and cleaned by @PeterH0323), which contains the annotation information required for training. The sample image is shown below:\n",
        "\n",
        "<div align=center>\n",
        "<img src=\"https://user-images.githubusercontent.com/25873202/205423220-c4b8f2fd-22ba-4937-8e47-1b3f6a8facd8.png\" alt=\"cat dataset\"/>\n",
        "</div>\n",
        "\n",
        "You can download and use it directly by the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMQXwWuIw3ef",
        "outputId": "c8efeac7-5b0c-4342-b5af-d3e790e358c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://download.openmmlab.com/mmyolo/data/cat_dataset.zip to data/cat/cat_dataset.zip\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217M/217M [00:19<00:00, 11.6MB/s]\n",
            "Unzipping cat_dataset.zip\n",
            "Delete data/cat/cat_dataset.zip\n"
          ]
        }
      ],
      "source": [
        "!python ../tools/misc/download_dataset.py --dataset-name cat --save-dir ../data/cat --unzip --delete"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "covQskXXw2ul"
      },
      "source": [
        "This dataset is automatically downloaded to the `./data/cat` dir with the following directory structure:\n",
        "\n",
        "<div align=center>\n",
        "<img src=\"https://user-images.githubusercontent.com/17425982/220072078-48b88a08-6179-483e-b8d3-0549e1b465de.png\" alt=\"image\"/>\n",
        "</div>\n",
        "\n",
        "The cat dataset is located in the mmyolo project dir, and `data/cat/annotations` stores annotations in COCO format, and `data/cat/images` stores all images\n",
        "\n",
        "## Config\n",
        "\n",
        "Taking YOLOv5 algorithm as an example, considering the limited GPU memory of users, we need to modify some default training parameters to make them run smoothly. The key parameters to be modified are as follows:\n",
        "\n",
        "- YOLOv5 is an Anchor-Based algorithm, and different datasets need to calculate suitable anchors adaptively\n",
        "- The default config uses 8 GPUs with a batch size of 16 per GPU. Now change it to a single GPU with a batch size of 12.\n",
        "- The default training epoch is 300. Change it to 40 epoch\n",
        "- Given the small size of the dataset, we opted to use fixed backbone weights\n",
        "- In principle, the learning rate should be linearly scaled accordingly when the batch size is changed, but actual measurements have found that this is not necessary\n",
        "\n",
        "Create a `yolov5_s-v61_fast_1xb12-40e_cat.py` config file in the `configs/yolov5` folder (we have provided this config for you to use directly) and copy the following into the config file.\n",
        "\n",
        "```python\n",
        "# Inherit and overwrite part of the config based on this config\n",
        "_base_ = 'yolov5_s-v61_syncbn_fast_8xb16-300e_coco.py'\n",
        "\n",
        "data_root = './data/cat/' # dataset root\n",
        "class_name = ('cat', ) # dataset category name\n",
        "num_classes = len(class_name) # dataset category number\n",
        "# metainfo is a configuration that must be passed to the dataloader, otherwise it is invalid\n",
        "# palette is a display color for category at visualization\n",
        "# The palette length must be greater than or equal to the length of the classes\n",
        "metainfo = dict(classes=class_name, palette=[(20, 220, 60)])\n",
        "\n",
        "# Adaptive anchor based on tools/analysis_tools/optimize_anchors.py\n",
        "anchors = [\n",
        "    [(68, 69), (154, 91), (143, 162)],  # P3/8\n",
        "    [(242, 160), (189, 287), (391, 207)],  # P4/16\n",
        "    [(353, 337), (539, 341), (443, 432)]  # P5/32\n",
        "]\n",
        "# Max training 40 epoch\n",
        "max_epochs = 40\n",
        "# bs = 12\n",
        "train_batch_size_per_gpu = 12\n",
        "# dataloader num workers\n",
        "train_num_workers = 4\n",
        "\n",
        "# load COCO pre-trained weight\n",
        "load_from = 'https://download.openmmlab.com/mmyolo/v0/yolov5/yolov5_s-v61_syncbn_fast_8xb16-300e_coco/yolov5_s-v61_syncbn_fast_8xb16-300e_coco_20220918_084700-86e02187.pth'  # noqa\n",
        "\n",
        "model = dict(\n",
        "    # Fixed the weight of the entire backbone without training\n",
        "    backbone=dict(frozen_stages=4),\n",
        "    bbox_head=dict(\n",
        "        head_module=dict(num_classes=num_classes),\n",
        "        prior_generator=dict(base_sizes=anchors)\n",
        "    ))\n",
        "\n",
        "train_dataloader = dict(\n",
        "    batch_size=train_batch_size_per_gpu,\n",
        "    num_workers=train_num_workers,\n",
        "    dataset=dict(\n",
        "        data_root=data_root,\n",
        "        metainfo=metainfo,\n",
        "        # Dataset annotation file of json path\n",
        "        ann_file='annotations/trainval.json',\n",
        "        # Dataset prefix\n",
        "        data_prefix=dict(img='images/')))\n",
        "\n",
        "val_dataloader = dict(\n",
        "    dataset=dict(\n",
        "        metainfo=metainfo,\n",
        "        data_root=data_root,\n",
        "        ann_file='annotations/test.json',\n",
        "        data_prefix=dict(img='images/')))\n",
        "\n",
        "test_dataloader = val_dataloader\n",
        "\n",
        "_base_.optim_wrapper.optimizer.batch_size_per_gpu = train_batch_size_per_gpu\n",
        "\n",
        "val_evaluator = dict(ann_file=data_root + 'annotations/test.json')\n",
        "test_evaluator = val_evaluator\n",
        "\n",
        "default_hooks = dict(\n",
        "    # Save weights every 10 epochs and a maximum of two weights can be saved.\n",
        "    # The best model is saved automatically during model evaluation\n",
        "    checkpoint=dict(interval=10, max_keep_ckpts=2, save_best='auto'),\n",
        "    # The warmup_mim_iter parameter is critical.\n",
        "    # The default value is 1000 which is not suitable for cat datasets.\n",
        "    param_scheduler=dict(max_epochs=max_epochs, warmup_mim_iter=10),\n",
        "    # The log printing interval is 5\n",
        "    logger=dict(type='LoggerHook', interval=5))\n",
        "# The evaluation interval is 10\n",
        "train_cfg = dict(max_epochs=max_epochs, val_interval=10)\n",
        "```\n",
        "\n",
        "The above config is inherited from `yolov5_s-v61_syncbn_fast_8xb16-300e_coco.py`. According to the characteristics of cat dataset updated `data_root`, `metainfo`, `train_dataloader`, `val_dataloader`, `num_classes` and other config.\n",
        "\n",
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "01/24 02:41:53 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmyolo\" in the \"log_processor\" registry tree. As a workaround, the current \"log_processor\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmyolo\" is a correct scope, or whether the registry is initialized.\n",
            "01/24 02:41:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.7.10 (default, Feb 26 2021, 18:47:35) [GCC 7.3.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 447225981\n",
            "    GPU 0: NVIDIA GeForce RTX 3090\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 11.1, V11.1.105\n",
            "    GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "    PyTorch: 1.9.0\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2021.2-Product Build 20210312 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 11.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37\n",
            "  - CuDNN 8.0.5\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "    TorchVision: 0.10.0\n",
            "    OpenCV: 4.9.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: True\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 447225981\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/24 02:41:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
            "_backend_args = None\n",
            "_multiscale_resize_transforms = [\n",
            "    dict(\n",
            "        transforms=[\n",
            "            dict(scale=(\n",
            "                640,\n",
            "                640,\n",
            "            ), type='YOLOv5KeepRatioResize'),\n",
            "            dict(\n",
            "                allow_scale_up=False,\n",
            "                pad_val=dict(img=114),\n",
            "                scale=(\n",
            "                    640,\n",
            "                    640,\n",
            "                ),\n",
            "                type='LetterResize'),\n",
            "        ],\n",
            "        type='Compose'),\n",
            "    dict(\n",
            "        transforms=[\n",
            "            dict(scale=(\n",
            "                320,\n",
            "                320,\n",
            "            ), type='YOLOv5KeepRatioResize'),\n",
            "            dict(\n",
            "                allow_scale_up=False,\n",
            "                pad_val=dict(img=114),\n",
            "                scale=(\n",
            "                    320,\n",
            "                    320,\n",
            "                ),\n",
            "                type='LetterResize'),\n",
            "        ],\n",
            "        type='Compose'),\n",
            "    dict(\n",
            "        transforms=[\n",
            "            dict(scale=(\n",
            "                960,\n",
            "                960,\n",
            "            ), type='YOLOv5KeepRatioResize'),\n",
            "            dict(\n",
            "                allow_scale_up=False,\n",
            "                pad_val=dict(img=114),\n",
            "                scale=(\n",
            "                    960,\n",
            "                    960,\n",
            "                ),\n",
            "                type='LetterResize'),\n",
            "        ],\n",
            "        type='Compose'),\n",
            "]\n",
            "affine_scale = 0.5\n",
            "albu_train_transforms = [\n",
            "    dict(p=0.01, type='Blur'),\n",
            "    dict(p=0.01, type='MedianBlur'),\n",
            "    dict(p=0.01, type='ToGray'),\n",
            "    dict(p=0.01, type='CLAHE'),\n",
            "]\n",
            "anchors = [\n",
            "    [\n",
            "        (\n",
            "            68,\n",
            "            69,\n",
            "        ),\n",
            "        (\n",
            "            154,\n",
            "            91,\n",
            "        ),\n",
            "        (\n",
            "            143,\n",
            "            162,\n",
            "        ),\n",
            "    ],\n",
            "    [\n",
            "        (\n",
            "            242,\n",
            "            160,\n",
            "        ),\n",
            "        (\n",
            "            189,\n",
            "            287,\n",
            "        ),\n",
            "        (\n",
            "            391,\n",
            "            207,\n",
            "        ),\n",
            "    ],\n",
            "    [\n",
            "        (\n",
            "            353,\n",
            "            337,\n",
            "        ),\n",
            "        (\n",
            "            539,\n",
            "            341,\n",
            "        ),\n",
            "        (\n",
            "            443,\n",
            "            432,\n",
            "        ),\n",
            "    ],\n",
            "]\n",
            "backend_args = None\n",
            "base_lr = 0.01\n",
            "batch_shapes_cfg = dict(\n",
            "    batch_size=1,\n",
            "    extra_pad_ratio=0.5,\n",
            "    img_size=640,\n",
            "    size_divisor=32,\n",
            "    type='BatchShapePolicy')\n",
            "class_name = ('cat', )\n",
            "custom_hooks = [\n",
            "    dict(\n",
            "        ema_type='ExpMomentumEMA',\n",
            "        momentum=0.0001,\n",
            "        priority=49,\n",
            "        strict_load=False,\n",
            "        type='EMAHook',\n",
            "        update_buffers=True),\n",
            "]\n",
            "data_root = './data/cat/'\n",
            "dataset_type = 'YOLOv5CocoDataset'\n",
            "deepen_factor = 0.33\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=10, max_keep_ckpts=2, save_best='auto',\n",
            "        type='CheckpointHook'),\n",
            "    logger=dict(interval=5, type='LoggerHook'),\n",
            "    param_scheduler=dict(\n",
            "        lr_factor=0.01,\n",
            "        max_epochs=40,\n",
            "        scheduler_type='linear',\n",
            "        type='YOLOv5ParamSchedulerHook',\n",
            "        warmup_mim_iter=10),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    timer=dict(type='IterTimerHook'),\n",
            "    visualization=dict(type='mmdet.DetVisualizationHook'))\n",
            "default_scope = 'mmyolo'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=True,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "img_scale = (\n",
            "    640,\n",
            "    640,\n",
            ")\n",
            "img_scales = [\n",
            "    (\n",
            "        640,\n",
            "        640,\n",
            "    ),\n",
            "    (\n",
            "        320,\n",
            "        320,\n",
            "    ),\n",
            "    (\n",
            "        960,\n",
            "        960,\n",
            "    ),\n",
            "]\n",
            "launcher = 'none'\n",
            "load_from = 'https://download.openmmlab.com/mmyolo/v0/yolov5/yolov5_s-v61_syncbn_fast_8xb16-300e_coco/yolov5_s-v61_syncbn_fast_8xb16-300e_coco_20220918_084700-86e02187.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
            "loss_bbox_weight = 0.05\n",
            "loss_cls_weight = 0.5\n",
            "loss_obj_weight = 1.0\n",
            "lr_factor = 0.01\n",
            "max_epochs = 40\n",
            "max_keep_ckpts = 3\n",
            "metainfo = dict(\n",
            "    classes=('cat', ), palette=[\n",
            "        (\n",
            "            20,\n",
            "            220,\n",
            "            60,\n",
            "        ),\n",
            "    ])\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        act_cfg=dict(inplace=True, type='SiLU'),\n",
            "        deepen_factor=0.33,\n",
            "        frozen_stages=4,\n",
            "        norm_cfg=dict(eps=0.001, momentum=0.03, type='BN'),\n",
            "        type='YOLOv5CSPDarknet',\n",
            "        widen_factor=0.5),\n",
            "    bbox_head=dict(\n",
            "        head_module=dict(\n",
            "            featmap_strides=[\n",
            "                8,\n",
            "                16,\n",
            "                32,\n",
            "            ],\n",
            "            in_channels=[\n",
            "                256,\n",
            "                512,\n",
            "                1024,\n",
            "            ],\n",
            "            num_base_priors=3,\n",
            "            num_classes=1,\n",
            "            type='YOLOv5HeadModule',\n",
            "            widen_factor=0.5),\n",
            "        loss_bbox=dict(\n",
            "            bbox_format='xywh',\n",
            "            eps=1e-07,\n",
            "            iou_mode='ciou',\n",
            "            loss_weight=0.05,\n",
            "            reduction='mean',\n",
            "            return_iou=True,\n",
            "            type='IoULoss'),\n",
            "        loss_cls=dict(\n",
            "            loss_weight=0.5,\n",
            "            reduction='mean',\n",
            "            type='mmdet.CrossEntropyLoss',\n",
            "            use_sigmoid=True),\n",
            "        loss_obj=dict(\n",
            "            loss_weight=1.0,\n",
            "            reduction='mean',\n",
            "            type='mmdet.CrossEntropyLoss',\n",
            "            use_sigmoid=True),\n",
            "        obj_level_weights=[\n",
            "            4.0,\n",
            "            1.0,\n",
            "            0.4,\n",
            "        ],\n",
            "        prior_generator=dict(\n",
            "            base_sizes=[\n",
            "                [\n",
            "                    (\n",
            "                        68,\n",
            "                        69,\n",
            "                    ),\n",
            "                    (\n",
            "                        154,\n",
            "                        91,\n",
            "                    ),\n",
            "                    (\n",
            "                        143,\n",
            "                        162,\n",
            "                    ),\n",
            "                ],\n",
            "                [\n",
            "                    (\n",
            "                        242,\n",
            "                        160,\n",
            "                    ),\n",
            "                    (\n",
            "                        189,\n",
            "                        287,\n",
            "                    ),\n",
            "                    (\n",
            "                        391,\n",
            "                        207,\n",
            "                    ),\n",
            "                ],\n",
            "                [\n",
            "                    (\n",
            "                        353,\n",
            "                        337,\n",
            "                    ),\n",
            "                    (\n",
            "                        539,\n",
            "                        341,\n",
            "                    ),\n",
            "                    (\n",
            "                        443,\n",
            "                        432,\n",
            "                    ),\n",
            "                ],\n",
            "            ],\n",
            "            strides=[\n",
            "                8,\n",
            "                16,\n",
            "                32,\n",
            "            ],\n",
            "            type='mmdet.YOLOAnchorGenerator'),\n",
            "        prior_match_thr=4.0,\n",
            "        type='YOLOv5Head'),\n",
            "    data_preprocessor=dict(\n",
            "        bgr_to_rgb=True,\n",
            "        mean=[\n",
            "            0.0,\n",
            "            0.0,\n",
            "            0.0,\n",
            "        ],\n",
            "        std=[\n",
            "            255.0,\n",
            "            255.0,\n",
            "            255.0,\n",
            "        ],\n",
            "        type='YOLOv5DetDataPreprocessor'),\n",
            "    neck=dict(\n",
            "        act_cfg=dict(inplace=True, type='SiLU'),\n",
            "        deepen_factor=0.33,\n",
            "        in_channels=[\n",
            "            256,\n",
            "            512,\n",
            "            1024,\n",
            "        ],\n",
            "        norm_cfg=dict(eps=0.001, momentum=0.03, type='BN'),\n",
            "        num_csp_blocks=3,\n",
            "        out_channels=[\n",
            "            256,\n",
            "            512,\n",
            "            1024,\n",
            "        ],\n",
            "        type='YOLOv5PAFPN',\n",
            "        widen_factor=0.5),\n",
            "    test_cfg=dict(\n",
            "        max_per_img=300,\n",
            "        multi_label=True,\n",
            "        nms=dict(iou_threshold=0.65, type='nms'),\n",
            "        nms_pre=30000,\n",
            "        score_thr=0.001),\n",
            "    type='YOLODetector')\n",
            "model_test_cfg = dict(\n",
            "    max_per_img=300,\n",
            "    multi_label=True,\n",
            "    nms=dict(iou_threshold=0.65, type='nms'),\n",
            "    nms_pre=30000,\n",
            "    score_thr=0.001)\n",
            "norm_cfg = dict(eps=0.001, momentum=0.03, type='BN')\n",
            "num_classes = 1\n",
            "num_det_layers = 3\n",
            "obj_level_weights = [\n",
            "    4.0,\n",
            "    1.0,\n",
            "    0.4,\n",
            "]\n",
            "optim_wrapper = dict(\n",
            "    constructor='YOLOv5OptimizerConstructor',\n",
            "    optimizer=dict(\n",
            "        batch_size_per_gpu=12,\n",
            "        lr=0.01,\n",
            "        momentum=0.937,\n",
            "        nesterov=True,\n",
            "        type='SGD',\n",
            "        weight_decay=0.0005),\n",
            "    type='OptimWrapper')\n",
            "param_scheduler = None\n",
            "persistent_workers = True\n",
            "pre_transform = [\n",
            "    dict(backend_args=None, type='LoadImageFromFile'),\n",
            "    dict(type='LoadAnnotations', with_bbox=True),\n",
            "]\n",
            "prior_match_thr = 4.0\n",
            "resume = False\n",
            "save_checkpoint_intervals = 10\n",
            "strides = [\n",
            "    8,\n",
            "    16,\n",
            "    32,\n",
            "]\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='annotations/test.json',\n",
            "        batch_shapes_cfg=dict(\n",
            "            batch_size=1,\n",
            "            extra_pad_ratio=0.5,\n",
            "            img_size=640,\n",
            "            size_divisor=32,\n",
            "            type='BatchShapePolicy'),\n",
            "        data_prefix=dict(img='images/'),\n",
            "        data_root='./data/cat/',\n",
            "        metainfo=dict(classes=('cat', ), palette=[\n",
            "            (\n",
            "                20,\n",
            "                220,\n",
            "                60,\n",
            "            ),\n",
            "        ]),\n",
            "        pipeline=[\n",
            "            dict(backend_args=None, type='LoadImageFromFile'),\n",
            "            dict(scale=(\n",
            "                640,\n",
            "                640,\n",
            "            ), type='YOLOv5KeepRatioResize'),\n",
            "            dict(\n",
            "                allow_scale_up=False,\n",
            "                pad_val=dict(img=114),\n",
            "                scale=(\n",
            "                    640,\n",
            "                    640,\n",
            "                ),\n",
            "                type='LetterResize'),\n",
            "            dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
            "            dict(\n",
            "                meta_keys=(\n",
            "                    'img_id',\n",
            "                    'img_path',\n",
            "                    'ori_shape',\n",
            "                    'img_shape',\n",
            "                    'scale_factor',\n",
            "                    'pad_param',\n",
            "                ),\n",
            "                type='mmdet.PackDetInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='YOLOv5CocoDataset'),\n",
            "    drop_last=False,\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    pin_memory=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = dict(\n",
            "    ann_file='./data/cat/annotations/test.json',\n",
            "    metric='bbox',\n",
            "    proposal_nums=(\n",
            "        100,\n",
            "        1,\n",
            "        10,\n",
            "    ),\n",
            "    type='mmdet.CocoMetric')\n",
            "test_pipeline = [\n",
            "    dict(backend_args=None, type='LoadImageFromFile'),\n",
            "    dict(scale=(\n",
            "        640,\n",
            "        640,\n",
            "    ), type='YOLOv5KeepRatioResize'),\n",
            "    dict(\n",
            "        allow_scale_up=False,\n",
            "        pad_val=dict(img=114),\n",
            "        scale=(\n",
            "            640,\n",
            "            640,\n",
            "        ),\n",
            "        type='LetterResize'),\n",
            "    dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
            "    dict(\n",
            "        meta_keys=(\n",
            "            'img_id',\n",
            "            'img_path',\n",
            "            'ori_shape',\n",
            "            'img_shape',\n",
            "            'scale_factor',\n",
            "            'pad_param',\n",
            "        ),\n",
            "        type='mmdet.PackDetInputs'),\n",
            "]\n",
            "train_ann_file = 'annotations/instances_train2017.json'\n",
            "train_batch_size_per_gpu = 12\n",
            "train_cfg = dict(max_epochs=40, type='EpochBasedTrainLoop', val_interval=10)\n",
            "train_data_prefix = 'train2017/'\n",
            "train_dataloader = dict(\n",
            "    batch_size=12,\n",
            "    collate_fn=dict(type='yolov5_collate'),\n",
            "    dataset=dict(\n",
            "        ann_file='annotations/trainval.json',\n",
            "        data_prefix=dict(img='images/'),\n",
            "        data_root='./data/cat/',\n",
            "        filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
            "        metainfo=dict(classes=('cat', ), palette=[\n",
            "            (\n",
            "                20,\n",
            "                220,\n",
            "                60,\n",
            "            ),\n",
            "        ]),\n",
            "        pipeline=[\n",
            "            dict(backend_args=None, type='LoadImageFromFile'),\n",
            "            dict(type='LoadAnnotations', with_bbox=True),\n",
            "            dict(\n",
            "                img_scale=(\n",
            "                    640,\n",
            "                    640,\n",
            "                ),\n",
            "                pad_val=114.0,\n",
            "                pre_transform=[\n",
            "                    dict(backend_args=None, type='LoadImageFromFile'),\n",
            "                    dict(type='LoadAnnotations', with_bbox=True),\n",
            "                ],\n",
            "                type='Mosaic'),\n",
            "            dict(\n",
            "                border=(\n",
            "                    -320,\n",
            "                    -320,\n",
            "                ),\n",
            "                border_val=(\n",
            "                    114,\n",
            "                    114,\n",
            "                    114,\n",
            "                ),\n",
            "                max_rotate_degree=0.0,\n",
            "                max_shear_degree=0.0,\n",
            "                scaling_ratio_range=(\n",
            "                    0.5,\n",
            "                    1.5,\n",
            "                ),\n",
            "                type='YOLOv5RandomAffine'),\n",
            "            dict(\n",
            "                bbox_params=dict(\n",
            "                    format='pascal_voc',\n",
            "                    label_fields=[\n",
            "                        'gt_bboxes_labels',\n",
            "                        'gt_ignore_flags',\n",
            "                    ],\n",
            "                    type='BboxParams'),\n",
            "                keymap=dict(gt_bboxes='bboxes', img='image'),\n",
            "                transforms=[\n",
            "                    dict(p=0.01, type='Blur'),\n",
            "                    dict(p=0.01, type='MedianBlur'),\n",
            "                    dict(p=0.01, type='ToGray'),\n",
            "                    dict(p=0.01, type='CLAHE'),\n",
            "                ],\n",
            "                type='mmdet.Albu'),\n",
            "            dict(type='YOLOv5HSVRandomAug'),\n",
            "            dict(prob=0.5, type='mmdet.RandomFlip'),\n",
            "            dict(\n",
            "                meta_keys=(\n",
            "                    'img_id',\n",
            "                    'img_path',\n",
            "                    'ori_shape',\n",
            "                    'img_shape',\n",
            "                    'flip',\n",
            "                    'flip_direction',\n",
            "                ),\n",
            "                type='mmdet.PackDetInputs'),\n",
            "        ],\n",
            "        type='YOLOv5CocoDataset'),\n",
            "    num_workers=4,\n",
            "    persistent_workers=True,\n",
            "    pin_memory=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_num_workers = 4\n",
            "train_pipeline = [\n",
            "    dict(backend_args=None, type='LoadImageFromFile'),\n",
            "    dict(type='LoadAnnotations', with_bbox=True),\n",
            "    dict(\n",
            "        img_scale=(\n",
            "            640,\n",
            "            640,\n",
            "        ),\n",
            "        pad_val=114.0,\n",
            "        pre_transform=[\n",
            "            dict(backend_args=None, type='LoadImageFromFile'),\n",
            "            dict(type='LoadAnnotations', with_bbox=True),\n",
            "        ],\n",
            "        type='Mosaic'),\n",
            "    dict(\n",
            "        border=(\n",
            "            -320,\n",
            "            -320,\n",
            "        ),\n",
            "        border_val=(\n",
            "            114,\n",
            "            114,\n",
            "            114,\n",
            "        ),\n",
            "        max_rotate_degree=0.0,\n",
            "        max_shear_degree=0.0,\n",
            "        scaling_ratio_range=(\n",
            "            0.5,\n",
            "            1.5,\n",
            "        ),\n",
            "        type='YOLOv5RandomAffine'),\n",
            "    dict(\n",
            "        bbox_params=dict(\n",
            "            format='pascal_voc',\n",
            "            label_fields=[\n",
            "                'gt_bboxes_labels',\n",
            "                'gt_ignore_flags',\n",
            "            ],\n",
            "            type='BboxParams'),\n",
            "        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
            "        transforms=[\n",
            "            dict(p=0.01, type='Blur'),\n",
            "            dict(p=0.01, type='MedianBlur'),\n",
            "            dict(p=0.01, type='ToGray'),\n",
            "            dict(p=0.01, type='CLAHE'),\n",
            "        ],\n",
            "        type='mmdet.Albu'),\n",
            "    dict(type='YOLOv5HSVRandomAug'),\n",
            "    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
            "    dict(\n",
            "        meta_keys=(\n",
            "            'img_id',\n",
            "            'img_path',\n",
            "            'ori_shape',\n",
            "            'img_shape',\n",
            "            'flip',\n",
            "            'flip_direction',\n",
            "        ),\n",
            "        type='mmdet.PackDetInputs'),\n",
            "]\n",
            "tta_model = dict(\n",
            "    tta_cfg=dict(max_per_img=300, nms=dict(iou_threshold=0.65, type='nms')),\n",
            "    type='mmdet.DetTTAModel')\n",
            "tta_pipeline = [\n",
            "    dict(backend_args=None, type='LoadImageFromFile'),\n",
            "    dict(\n",
            "        transforms=[\n",
            "            [\n",
            "                dict(\n",
            "                    transforms=[\n",
            "                        dict(scale=(\n",
            "                            640,\n",
            "                            640,\n",
            "                        ), type='YOLOv5KeepRatioResize'),\n",
            "                        dict(\n",
            "                            allow_scale_up=False,\n",
            "                            pad_val=dict(img=114),\n",
            "                            scale=(\n",
            "                                640,\n",
            "                                640,\n",
            "                            ),\n",
            "                            type='LetterResize'),\n",
            "                    ],\n",
            "                    type='Compose'),\n",
            "                dict(\n",
            "                    transforms=[\n",
            "                        dict(scale=(\n",
            "                            320,\n",
            "                            320,\n",
            "                        ), type='YOLOv5KeepRatioResize'),\n",
            "                        dict(\n",
            "                            allow_scale_up=False,\n",
            "                            pad_val=dict(img=114),\n",
            "                            scale=(\n",
            "                                320,\n",
            "                                320,\n",
            "                            ),\n",
            "                            type='LetterResize'),\n",
            "                    ],\n",
            "                    type='Compose'),\n",
            "                dict(\n",
            "                    transforms=[\n",
            "                        dict(scale=(\n",
            "                            960,\n",
            "                            960,\n",
            "                        ), type='YOLOv5KeepRatioResize'),\n",
            "                        dict(\n",
            "                            allow_scale_up=False,\n",
            "                            pad_val=dict(img=114),\n",
            "                            scale=(\n",
            "                                960,\n",
            "                                960,\n",
            "                            ),\n",
            "                            type='LetterResize'),\n",
            "                    ],\n",
            "                    type='Compose'),\n",
            "            ],\n",
            "            [\n",
            "                dict(prob=1.0, type='mmdet.RandomFlip'),\n",
            "                dict(prob=0.0, type='mmdet.RandomFlip'),\n",
            "            ],\n",
            "            [\n",
            "                dict(type='mmdet.LoadAnnotations', with_bbox=True),\n",
            "            ],\n",
            "            [\n",
            "                dict(\n",
            "                    meta_keys=(\n",
            "                        'img_id',\n",
            "                        'img_path',\n",
            "                        'ori_shape',\n",
            "                        'img_shape',\n",
            "                        'scale_factor',\n",
            "                        'pad_param',\n",
            "                        'flip',\n",
            "                        'flip_direction',\n",
            "                    ),\n",
            "                    type='mmdet.PackDetInputs'),\n",
            "            ],\n",
            "        ],\n",
            "        type='TestTimeAug'),\n",
            "]\n",
            "val_ann_file = 'annotations/instances_val2017.json'\n",
            "val_batch_size_per_gpu = 1\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_data_prefix = 'val2017/'\n",
            "val_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='annotations/test.json',\n",
            "        batch_shapes_cfg=dict(\n",
            "            batch_size=1,\n",
            "            extra_pad_ratio=0.5,\n",
            "            img_size=640,\n",
            "            size_divisor=32,\n",
            "            type='BatchShapePolicy'),\n",
            "        data_prefix=dict(img='images/'),\n",
            "        data_root='./data/cat/',\n",
            "        metainfo=dict(classes=('cat', ), palette=[\n",
            "            (\n",
            "                20,\n",
            "                220,\n",
            "                60,\n",
            "            ),\n",
            "        ]),\n",
            "        pipeline=[\n",
            "            dict(backend_args=None, type='LoadImageFromFile'),\n",
            "            dict(scale=(\n",
            "                640,\n",
            "                640,\n",
            "            ), type='YOLOv5KeepRatioResize'),\n",
            "            dict(\n",
            "                allow_scale_up=False,\n",
            "                pad_val=dict(img=114),\n",
            "                scale=(\n",
            "                    640,\n",
            "                    640,\n",
            "                ),\n",
            "                type='LetterResize'),\n",
            "            dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
            "            dict(\n",
            "                meta_keys=(\n",
            "                    'img_id',\n",
            "                    'img_path',\n",
            "                    'ori_shape',\n",
            "                    'img_shape',\n",
            "                    'scale_factor',\n",
            "                    'pad_param',\n",
            "                ),\n",
            "                type='mmdet.PackDetInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='YOLOv5CocoDataset'),\n",
            "    drop_last=False,\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    pin_memory=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(\n",
            "    ann_file='./data/cat/annotations/test.json',\n",
            "    metric='bbox',\n",
            "    proposal_nums=(\n",
            "        100,\n",
            "        1,\n",
            "        10,\n",
            "    ),\n",
            "    type='mmdet.CocoMetric')\n",
            "val_num_workers = 2\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    name='visualizer',\n",
            "    type='mmdet.DetLocalVisualizer',\n",
            "    vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "weight_decay = 0.0005\n",
            "widen_factor = 0.5\n",
            "work_dir = './work_dirs/yolov5_s-v61_fast_1xb12-40e_cat'\n",
            "\n",
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "01/24 02:41:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/24 02:41:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(49          ) EMAHook                            \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_load_checkpoint:\n",
            "(49          ) EMAHook                            \n",
            " -------------------- \n",
            "before_train:\n",
            "(9           ) YOLOv5ParamSchedulerHook           \n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(49          ) EMAHook                            \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(9           ) YOLOv5ParamSchedulerHook           \n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(9           ) YOLOv5ParamSchedulerHook           \n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(49          ) EMAHook                            \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(9           ) YOLOv5ParamSchedulerHook           \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(49          ) EMAHook                            \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DetVisualizationHook               \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(9           ) YOLOv5ParamSchedulerHook           \n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(49          ) EMAHook                            \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_save_checkpoint:\n",
            "(49          ) EMAHook                            \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(49          ) EMAHook                            \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DetVisualizationHook               \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(49          ) EMAHook                            \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "01/24 02:41:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Scaled weight_decay to 0.00046875\n",
            "01/24 02:41:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Optimizer groups: 60 .bias, 60 conv.weight, 57 other\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmyolo/v0/yolov5/yolov5_s-v61_syncbn_fast_8xb16-300e_coco/yolov5_s-v61_syncbn_fast_8xb16-300e_coco_20220918_084700-86e02187.pth\n",
            "Downloading: \"https://download.openmmlab.com/mmyolo/v0/yolov5/yolov5_s-v61_syncbn_fast_8xb16-300e_coco/yolov5_s-v61_syncbn_fast_8xb16-300e_coco_20220918_084700-86e02187.pth\" to /root/.cache/torch/hub/checkpoints/yolov5_s-v61_syncbn_fast_8xb16-300e_coco_20220918_084700-86e02187.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27.8M/27.8M [00:02<00:00, 10.4MB/s]\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for bbox_head.head_module.convs_pred.0.weight: copying a param with shape torch.Size([255, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([18, 128, 1, 1]).\n",
            "size mismatch for bbox_head.head_module.convs_pred.0.bias: copying a param with shape torch.Size([255]) from checkpoint, the shape in current model is torch.Size([18]).\n",
            "size mismatch for bbox_head.head_module.convs_pred.1.weight: copying a param with shape torch.Size([255, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([18, 256, 1, 1]).\n",
            "size mismatch for bbox_head.head_module.convs_pred.1.bias: copying a param with shape torch.Size([255]) from checkpoint, the shape in current model is torch.Size([18]).\n",
            "size mismatch for bbox_head.head_module.convs_pred.2.weight: copying a param with shape torch.Size([255, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([18, 512, 1, 1]).\n",
            "size mismatch for bbox_head.head_module.convs_pred.2.bias: copying a param with shape torch.Size([255]) from checkpoint, the shape in current model is torch.Size([18]).\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for bbox_head.head_module.convs_pred.0.weight: copying a param with shape torch.Size([255, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([18, 128, 1, 1]).\n",
            "size mismatch for bbox_head.head_module.convs_pred.0.bias: copying a param with shape torch.Size([255]) from checkpoint, the shape in current model is torch.Size([18]).\n",
            "size mismatch for bbox_head.head_module.convs_pred.1.weight: copying a param with shape torch.Size([255, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([18, 256, 1, 1]).\n",
            "size mismatch for bbox_head.head_module.convs_pred.1.bias: copying a param with shape torch.Size([255]) from checkpoint, the shape in current model is torch.Size([18]).\n",
            "size mismatch for bbox_head.head_module.convs_pred.2.weight: copying a param with shape torch.Size([255, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([18, 512, 1, 1]).\n",
            "size mismatch for bbox_head.head_module.convs_pred.2.bias: copying a param with shape torch.Size([255]) from checkpoint, the shape in current model is torch.Size([18]).\n",
            "01/24 02:42:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from https://download.openmmlab.com/mmyolo/v0/yolov5/yolov5_s-v61_syncbn_fast_8xb16-300e_coco/yolov5_s-v61_syncbn_fast_8xb16-300e_coco_20220918_084700-86e02187.pth\n",
            "01/24 02:42:01 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
            "01/24 02:42:01 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
            "01/24 02:42:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /mmyolo/work_dirs/yolov5_s-v61_fast_1xb12-40e_cat.\n",
            "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "01/24 02:42:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][ 5/10]  base_lr: 1.0000e-02 lr: 1.3333e-03  eta: 0:11:15  time: 1.7110  data_time: 1.2911  memory: 4669  loss: 1.8104  loss_cls: 0.0000  loss_obj: 0.4362  loss_bbox: 1.3743\n",
            "01/24 02:42:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:42:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][10/10]  base_lr: 1.0000e-02 lr: 3.0000e-03  eta: 0:08:40  time: 1.3336  data_time: 1.0329  memory: 4604  loss: 1.6946  loss_cls: 0.0000  loss_obj: 0.4351  loss_bbox: 1.2595\n",
            "01/24 02:42:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [2][ 5/10]  base_lr: 1.0000e-02 lr: 4.5512e-03  eta: 0:09:21  time: 1.4577  data_time: 1.2388  memory: 1219  loss: 1.6315  loss_cls: 0.0000  loss_obj: 0.4347  loss_bbox: 1.1967\n",
            "01/24 02:42:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:42:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [2][10/10]  base_lr: 1.0000e-02 lr: 6.1766e-03  eta: 0:08:23  time: 1.3254  data_time: 1.1491  memory: 1219  loss: 1.5280  loss_cls: 0.0000  loss_obj: 0.4254  loss_bbox: 1.1026\n",
            "01/24 02:42:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [3][ 5/10]  base_lr: 1.0000e-02 lr: 7.6040e-03  eta: 0:08:46  time: 1.4039  data_time: 1.2494  memory: 1219  loss: 1.4617  loss_cls: 0.0000  loss_obj: 0.4225  loss_bbox: 1.0392\n",
            "01/24 02:42:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:42:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [3][10/10]  base_lr: 1.0000e-02 lr: 9.1882e-03  eta: 0:08:04  time: 1.3092  data_time: 1.1699  memory: 1219  loss: 1.4014  loss_cls: 0.0000  loss_obj: 0.4153  loss_bbox: 0.9861\n",
            "01/24 02:42:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [4][ 5/10]  base_lr: 1.0000e-02 lr: 9.2575e-03  eta: 0:08:20  time: 1.3722  data_time: 1.2445  memory: 1219  loss: 1.3548  loss_cls: 0.0000  loss_obj: 0.4151  loss_bbox: 0.9397\n",
            "01/24 02:42:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:42:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [4][10/10]  base_lr: 1.0000e-02 lr: 9.2575e-03  eta: 0:07:57  time: 1.3251  data_time: 1.2069  memory: 1219  loss: 1.3060  loss_cls: 0.0000  loss_obj: 0.4138  loss_bbox: 0.8922\n",
            "01/24 02:43:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [5][ 5/10]  base_lr: 1.0000e-02 lr: 9.2575e-03  eta: 0:08:07  time: 1.3728  data_time: 1.2589  memory: 1219  loss: 1.2711  loss_cls: 0.0000  loss_obj: 0.4133  loss_bbox: 0.8578\n",
            "01/24 02:43:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:43:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [5][10/10]  base_lr: 1.0000e-02 lr: 9.2575e-03  eta: 0:07:39  time: 1.3134  data_time: 1.2054  memory: 1219  loss: 1.2294  loss_cls: 0.0000  loss_obj: 0.4042  loss_bbox: 0.8253\n",
            "01/24 02:43:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [6][ 5/10]  base_lr: 1.0000e-02 lr: 9.0100e-03  eta: 0:07:48  time: 1.3228  data_time: 1.2492  memory: 1219  loss: 1.1319  loss_cls: 0.0000  loss_obj: 0.3932  loss_bbox: 0.7386\n",
            "01/24 02:43:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:43:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [6][10/10]  base_lr: 1.0000e-02 lr: 9.0100e-03  eta: 0:07:26  time: 1.3090  data_time: 1.2475  memory: 1219  loss: 1.0577  loss_cls: 0.0000  loss_obj: 0.3828  loss_bbox: 0.6749\n",
            "01/24 02:43:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [7][ 5/10]  base_lr: 1.0000e-02 lr: 8.7625e-03  eta: 0:07:34  time: 1.3263  data_time: 1.2651  memory: 1219  loss: 0.9901  loss_cls: 0.0000  loss_obj: 0.3691  loss_bbox: 0.6210\n",
            "01/24 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [7][10/10]  base_lr: 1.0000e-02 lr: 8.7625e-03  eta: 0:07:15  time: 1.3162  data_time: 1.2550  memory: 1219  loss: 0.9424  loss_cls: 0.0000  loss_obj: 0.3543  loss_bbox: 0.5881\n",
            "01/24 02:43:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [8][ 5/10]  base_lr: 1.0000e-02 lr: 8.5150e-03  eta: 0:07:18  time: 1.3225  data_time: 1.2625  memory: 1219  loss: 0.8994  loss_cls: 0.0000  loss_obj: 0.3404  loss_bbox: 0.5589\n",
            "01/24 02:43:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:43:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [8][10/10]  base_lr: 1.0000e-02 lr: 8.5150e-03  eta: 0:07:01  time: 1.3207  data_time: 1.2621  memory: 1219  loss: 0.8561  loss_cls: 0.0000  loss_obj: 0.3272  loss_bbox: 0.5289\n",
            "01/24 02:43:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [9][ 5/10]  base_lr: 1.0000e-02 lr: 8.2675e-03  eta: 0:07:05  time: 1.3345  data_time: 1.2763  memory: 1219  loss: 0.8233  loss_cls: 0.0000  loss_obj: 0.3157  loss_bbox: 0.5076\n",
            "01/24 02:44:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:44:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [9][10/10]  base_lr: 1.0000e-02 lr: 8.2675e-03  eta: 0:06:49  time: 1.3177  data_time: 1.2600  memory: 1219  loss: 0.7942  loss_cls: 0.0000  loss_obj: 0.3013  loss_bbox: 0.4930\n",
            "01/24 02:44:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][ 5/10]  base_lr: 1.0000e-02 lr: 8.0200e-03  eta: 0:06:47  time: 1.3008  data_time: 1.2453  memory: 1219  loss: 0.7661  loss_cls: 0.0000  loss_obj: 0.2863  loss_bbox: 0.4797\n",
            "01/24 02:44:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:44:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][10/10]  base_lr: 1.0000e-02 lr: 8.0200e-03  eta: 0:06:33  time: 1.3122  data_time: 1.2573  memory: 1219  loss: 0.7448  loss_cls: 0.0000  loss_obj: 0.2774  loss_bbox: 0.4674\n",
            "01/24 02:44:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 10 epochs\n",
            "01/24 02:44:13 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `save_param_scheduler` is True but `self.param_schedulers` is None, so skip saving parameter schedulers\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "01/24 02:44:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [10][ 5/28]    eta: 0:00:05  time: 0.2409  data_time: 0.1029  memory: 4449  \n",
            "01/24 02:44:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [10][10/28]    eta: 0:00:02  time: 0.1661  data_time: 0.0622  memory: 4453  \n",
            "01/24 02:44:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [10][15/28]    eta: 0:00:01  time: 0.1505  data_time: 0.0425  memory: 4449  \n",
            "01/24 02:44:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [10][20/28]    eta: 0:00:01  time: 0.1286  data_time: 0.0359  memory: 116  \n",
            "01/24 02:44:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [10][25/28]    eta: 0:00:00  time: 0.1212  data_time: 0.0359  memory: 116  \n",
            "01/24 02:44:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.05s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.393\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.704\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.406\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.467\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.653\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.670\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.670\n",
            "01/24 02:44:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.393 0.704 0.406 -1.000 -1.000 0.393\n",
            "01/24 02:44:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [10][28/28]    coco/bbox_mAP: 0.3930  coco/bbox_mAP_50: 0.7040  coco/bbox_mAP_75: 0.4060  coco/bbox_mAP_s: -1.0000  coco/bbox_mAP_m: -1.0000  coco/bbox_mAP_l: 0.3930  data_time: 0.0321  time: 0.1123\n",
            "01/24 02:44:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The best checkpoint with 0.3930 coco/bbox_mAP at 10 epoch is saved to best_coco_bbox_mAP_epoch_10.pth.\n",
            "01/24 02:44:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [11][ 5/10]  base_lr: 1.0000e-02 lr: 7.7725e-03  eta: 0:06:33  time: 1.3051  data_time: 1.2520  memory: 1226  loss: 0.7261  loss_cls: 0.0000  loss_obj: 0.2674  loss_bbox: 0.4587\n",
            "01/24 02:44:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:44:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [11][10/10]  base_lr: 1.0000e-02 lr: 7.7725e-03  eta: 0:06:20  time: 1.3140  data_time: 1.2611  memory: 1226  loss: 0.7062  loss_cls: 0.0000  loss_obj: 0.2592  loss_bbox: 0.4470\n",
            "01/24 02:44:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [12][ 5/10]  base_lr: 1.0000e-02 lr: 7.5250e-03  eta: 0:06:20  time: 1.3063  data_time: 1.2510  memory: 1226  loss: 0.6891  loss_cls: 0.0000  loss_obj: 0.2551  loss_bbox: 0.4340\n",
            "01/24 02:44:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:44:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [12][10/10]  base_lr: 1.0000e-02 lr: 7.5250e-03  eta: 0:06:08  time: 1.3126  data_time: 1.2557  memory: 1226  loss: 0.6730  loss_cls: 0.0000  loss_obj: 0.2522  loss_bbox: 0.4209\n",
            "01/24 02:44:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [13][ 5/10]  base_lr: 1.0000e-02 lr: 7.2775e-03  eta: 0:06:08  time: 1.3235  data_time: 1.2673  memory: 1226  loss: 0.6595  loss_cls: 0.0000  loss_obj: 0.2494  loss_bbox: 0.4101\n",
            "01/24 02:44:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:44:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [13][10/10]  base_lr: 1.0000e-02 lr: 7.2775e-03  eta: 0:05:56  time: 1.3281  data_time: 1.2717  memory: 1226  loss: 0.6514  loss_cls: 0.0000  loss_obj: 0.2476  loss_bbox: 0.4038\n",
            "01/24 02:45:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [14][ 5/10]  base_lr: 1.0000e-02 lr: 7.0300e-03  eta: 0:05:57  time: 1.3445  data_time: 1.2872  memory: 1226  loss: 0.6371  loss_cls: 0.0000  loss_obj: 0.2421  loss_bbox: 0.3950\n",
            "01/24 02:45:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:45:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [14][10/10]  base_lr: 1.0000e-02 lr: 7.0300e-03  eta: 0:05:47  time: 1.3636  data_time: 1.3056  memory: 1226  loss: 0.6292  loss_cls: 0.0000  loss_obj: 0.2384  loss_bbox: 0.3908\n",
            "01/24 02:45:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [15][ 5/10]  base_lr: 1.0000e-02 lr: 6.7825e-03  eta: 0:05:43  time: 1.3728  data_time: 1.3152  memory: 1226  loss: 0.6227  loss_cls: 0.0000  loss_obj: 0.2377  loss_bbox: 0.3850\n",
            "01/24 02:45:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:45:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [15][10/10]  base_lr: 1.0000e-02 lr: 6.7825e-03  eta: 0:05:32  time: 1.3605  data_time: 1.3033  memory: 1226  loss: 0.6153  loss_cls: 0.0000  loss_obj: 0.2381  loss_bbox: 0.3773\n",
            "01/24 02:45:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [16][ 5/10]  base_lr: 1.0000e-02 lr: 6.5350e-03  eta: 0:05:28  time: 1.3537  data_time: 1.2956  memory: 1226  loss: 0.6131  loss_cls: 0.0000  loss_obj: 0.2408  loss_bbox: 0.3724\n",
            "01/24 02:45:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:45:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [16][10/10]  base_lr: 1.0000e-02 lr: 6.5350e-03  eta: 0:05:17  time: 1.3454  data_time: 1.2876  memory: 1226  loss: 0.6107  loss_cls: 0.0000  loss_obj: 0.2423  loss_bbox: 0.3684\n",
            "01/24 02:45:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [17][ 5/10]  base_lr: 1.0000e-02 lr: 6.2875e-03  eta: 0:05:15  time: 1.3569  data_time: 1.3003  memory: 1226  loss: 0.6041  loss_cls: 0.0000  loss_obj: 0.2410  loss_bbox: 0.3631\n",
            "01/24 02:45:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:45:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [17][10/10]  base_lr: 1.0000e-02 lr: 6.2875e-03  eta: 0:05:05  time: 1.3511  data_time: 1.2957  memory: 1226  loss: 0.5985  loss_cls: 0.0000  loss_obj: 0.2380  loss_bbox: 0.3605\n",
            "01/24 02:46:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [18][ 5/10]  base_lr: 1.0000e-02 lr: 6.0400e-03  eta: 0:05:00  time: 1.3338  data_time: 1.2781  memory: 1226  loss: 0.5905  loss_cls: 0.0000  loss_obj: 0.2363  loss_bbox: 0.3541\n",
            "01/24 02:46:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:46:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [18][10/10]  base_lr: 1.0000e-02 lr: 6.0400e-03  eta: 0:04:50  time: 1.3274  data_time: 1.2724  memory: 1226  loss: 0.5867  loss_cls: 0.0000  loss_obj: 0.2344  loss_bbox: 0.3523\n",
            "01/24 02:46:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [19][ 5/10]  base_lr: 1.0000e-02 lr: 5.7925e-03  eta: 0:04:46  time: 1.2904  data_time: 1.2358  memory: 1226  loss: 0.5791  loss_cls: 0.0000  loss_obj: 0.2306  loss_bbox: 0.3485\n",
            "01/24 02:46:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:46:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [19][10/10]  base_lr: 1.0000e-02 lr: 5.7925e-03  eta: 0:04:36  time: 1.2692  data_time: 1.2149  memory: 1226  loss: 0.5706  loss_cls: 0.0000  loss_obj: 0.2285  loss_bbox: 0.3420\n",
            "01/24 02:46:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [20][ 5/10]  base_lr: 1.0000e-02 lr: 5.5450e-03  eta: 0:04:31  time: 1.2625  data_time: 1.2083  memory: 1226  loss: 0.5632  loss_cls: 0.0000  loss_obj: 0.2265  loss_bbox: 0.3367\n",
            "01/24 02:46:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:46:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [20][10/10]  base_lr: 1.0000e-02 lr: 5.5450e-03  eta: 0:04:22  time: 1.2665  data_time: 1.2117  memory: 1226  loss: 0.5598  loss_cls: 0.0000  loss_obj: 0.2248  loss_bbox: 0.3349\n",
            "01/24 02:46:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 20 epochs\n",
            "01/24 02:46:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [20][ 5/28]    eta: 0:00:03  time: 0.1180  data_time: 0.0450  memory: 116  \n",
            "01/24 02:46:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [20][10/28]    eta: 0:00:01  time: 0.1091  data_time: 0.0416  memory: 129  \n",
            "01/24 02:46:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [20][15/28]    eta: 0:00:01  time: 0.1065  data_time: 0.0433  memory: 129  \n",
            "01/24 02:46:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [20][20/28]    eta: 0:00:00  time: 0.1043  data_time: 0.0450  memory: 116  \n",
            "01/24 02:46:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [20][25/28]    eta: 0:00:00  time: 0.0845  data_time: 0.0369  memory: 116  \n",
            "01/24 02:46:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.03s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.665\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.970\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.815\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.665\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.607\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.740\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.750\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.750\n",
            "01/24 02:46:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.665 0.970 0.815 -1.000 -1.000 0.665\n",
            "01/24 02:46:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [20][28/28]    coco/bbox_mAP: 0.6650  coco/bbox_mAP_50: 0.9700  coco/bbox_mAP_75: 0.8150  coco/bbox_mAP_s: -1.0000  coco/bbox_mAP_m: -1.0000  coco/bbox_mAP_l: 0.6650  data_time: 0.0539  time: 0.0859\n",
            "01/24 02:46:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The previous best checkpoint /mmyolo/work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/best_coco_bbox_mAP_epoch_10.pth is removed\n",
            "01/24 02:46:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The best checkpoint with 0.6650 coco/bbox_mAP at 20 epoch is saved to best_coco_bbox_mAP_epoch_20.pth.\n",
            "01/24 02:46:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [21][ 5/10]  base_lr: 1.0000e-02 lr: 5.2975e-03  eta: 0:04:18  time: 1.2868  data_time: 1.2334  memory: 1226  loss: 0.5533  loss_cls: 0.0000  loss_obj: 0.2223  loss_bbox: 0.3310\n",
            "01/24 02:46:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:46:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [21][10/10]  base_lr: 1.0000e-02 lr: 5.2975e-03  eta: 0:04:09  time: 1.2892  data_time: 1.2363  memory: 1226  loss: 0.5448  loss_cls: 0.0000  loss_obj: 0.2180  loss_bbox: 0.3268\n",
            "01/24 02:46:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [22][ 5/10]  base_lr: 1.0000e-02 lr: 5.0500e-03  eta: 0:04:05  time: 1.2737  data_time: 1.2221  memory: 1226  loss: 0.5405  loss_cls: 0.0000  loss_obj: 0.2172  loss_bbox: 0.3234\n",
            "01/24 02:47:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:47:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [22][10/10]  base_lr: 1.0000e-02 lr: 5.0500e-03  eta: 0:03:56  time: 1.2707  data_time: 1.2197  memory: 1226  loss: 0.5402  loss_cls: 0.0000  loss_obj: 0.2210  loss_bbox: 0.3192\n",
            "01/24 02:47:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [23][ 5/10]  base_lr: 1.0000e-02 lr: 4.8025e-03  eta: 0:03:51  time: 1.2710  data_time: 1.2203  memory: 1226  loss: 0.5385  loss_cls: 0.0000  loss_obj: 0.2214  loss_bbox: 0.3171\n",
            "01/24 02:47:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:47:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [23][10/10]  base_lr: 1.0000e-02 lr: 4.8025e-03  eta: 0:03:43  time: 1.2744  data_time: 1.2232  memory: 1226  loss: 0.5308  loss_cls: 0.0000  loss_obj: 0.2192  loss_bbox: 0.3116\n",
            "01/24 02:47:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [24][ 5/10]  base_lr: 1.0000e-02 lr: 4.5550e-03  eta: 0:03:37  time: 1.2797  data_time: 1.2282  memory: 1226  loss: 0.5308  loss_cls: 0.0000  loss_obj: 0.2223  loss_bbox: 0.3084\n",
            "01/24 02:47:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:47:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [24][10/10]  base_lr: 1.0000e-02 lr: 4.5550e-03  eta: 0:03:29  time: 1.2847  data_time: 1.2333  memory: 1226  loss: 0.5291  loss_cls: 0.0000  loss_obj: 0.2230  loss_bbox: 0.3061\n",
            "01/24 02:47:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [25][ 5/10]  base_lr: 1.0000e-02 lr: 4.3075e-03  eta: 0:03:25  time: 1.3183  data_time: 1.2657  memory: 1226  loss: 0.5256  loss_cls: 0.0000  loss_obj: 0.2206  loss_bbox: 0.3049\n",
            "01/24 02:47:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:47:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [25][10/10]  base_lr: 1.0000e-02 lr: 4.3075e-03  eta: 0:03:17  time: 1.3347  data_time: 1.2821  memory: 1226  loss: 0.5218  loss_cls: 0.0000  loss_obj: 0.2199  loss_bbox: 0.3019\n",
            "01/24 02:47:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [26][ 5/10]  base_lr: 1.0000e-02 lr: 4.0600e-03  eta: 0:03:12  time: 1.3434  data_time: 1.2910  memory: 1226  loss: 0.5180  loss_cls: 0.0000  loss_obj: 0.2198  loss_bbox: 0.2982\n",
            "01/24 02:47:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:47:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [26][10/10]  base_lr: 1.0000e-02 lr: 4.0600e-03  eta: 0:03:05  time: 1.3619  data_time: 1.3090  memory: 1226  loss: 0.5100  loss_cls: 0.0000  loss_obj: 0.2165  loss_bbox: 0.2935\n",
            "01/24 02:48:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [27][ 5/10]  base_lr: 1.0000e-02 lr: 3.8125e-03  eta: 0:03:00  time: 1.3676  data_time: 1.3135  memory: 1226  loss: 0.5048  loss_cls: 0.0000  loss_obj: 0.2148  loss_bbox: 0.2900\n",
            "01/24 02:48:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:48:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [27][10/10]  base_lr: 1.0000e-02 lr: 3.8125e-03  eta: 0:02:52  time: 1.3772  data_time: 1.3217  memory: 1226  loss: 0.4982  loss_cls: 0.0000  loss_obj: 0.2129  loss_bbox: 0.2853\n",
            "01/24 02:48:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [28][ 5/10]  base_lr: 1.0000e-02 lr: 3.5650e-03  eta: 0:02:46  time: 1.3798  data_time: 1.3241  memory: 1226  loss: 0.4947  loss_cls: 0.0000  loss_obj: 0.2124  loss_bbox: 0.2823\n",
            "01/24 02:48:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:48:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [28][10/10]  base_lr: 1.0000e-02 lr: 3.5650e-03  eta: 0:02:38  time: 1.3820  data_time: 1.3260  memory: 1226  loss: 0.4926  loss_cls: 0.0000  loss_obj: 0.2134  loss_bbox: 0.2791\n",
            "01/24 02:48:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [29][ 5/10]  base_lr: 1.0000e-02 lr: 3.3175e-03  eta: 0:02:33  time: 1.3764  data_time: 1.3212  memory: 1226  loss: 0.4877  loss_cls: 0.0000  loss_obj: 0.2116  loss_bbox: 0.2761\n",
            "01/24 02:48:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:48:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [29][10/10]  base_lr: 1.0000e-02 lr: 3.3175e-03  eta: 0:02:25  time: 1.3690  data_time: 1.3139  memory: 1226  loss: 0.4840  loss_cls: 0.0000  loss_obj: 0.2094  loss_bbox: 0.2746\n",
            "01/24 02:48:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [30][ 5/10]  base_lr: 1.0000e-02 lr: 3.0700e-03  eta: 0:02:19  time: 1.3567  data_time: 1.3028  memory: 1226  loss: 0.4807  loss_cls: 0.0000  loss_obj: 0.2103  loss_bbox: 0.2703\n",
            "01/24 02:48:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:48:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [30][10/10]  base_lr: 1.0000e-02 lr: 3.0700e-03  eta: 0:02:12  time: 1.3401  data_time: 1.2860  memory: 1226  loss: 0.4805  loss_cls: 0.0000  loss_obj: 0.2125  loss_bbox: 0.2680\n",
            "01/24 02:48:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 30 epochs\n",
            "01/24 02:48:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [30][ 5/28]    eta: 0:00:03  time: 0.0943  data_time: 0.0506  memory: 116  \n",
            "01/24 02:48:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [30][10/28]    eta: 0:00:01  time: 0.0857  data_time: 0.0513  memory: 129  \n",
            "01/24 02:48:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [30][15/28]    eta: 0:00:01  time: 0.0893  data_time: 0.0560  memory: 129  \n",
            "01/24 02:48:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [30][20/28]    eta: 0:00:00  time: 0.0863  data_time: 0.0558  memory: 116  \n",
            "01/24 02:48:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [30][25/28]    eta: 0:00:00  time: 0.0798  data_time: 0.0496  memory: 116  \n",
            "01/24 02:48:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.02s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.00s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.733\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.971\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.869\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.733\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.683\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.807\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.807\n",
            "01/24 02:48:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.733 0.971 0.869 -1.000 -1.000 0.733\n",
            "01/24 02:48:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [30][28/28]    coco/bbox_mAP: 0.7330  coco/bbox_mAP_50: 0.9710  coco/bbox_mAP_75: 0.8690  coco/bbox_mAP_s: -1.0000  coco/bbox_mAP_m: -1.0000  coco/bbox_mAP_l: 0.7330  data_time: 0.0505  time: 0.0792\n",
            "01/24 02:48:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The previous best checkpoint /mmyolo/work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/best_coco_bbox_mAP_epoch_20.pth is removed\n",
            "01/24 02:48:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The best checkpoint with 0.7330 coco/bbox_mAP at 30 epoch is saved to best_coco_bbox_mAP_epoch_30.pth.\n",
            "01/24 02:49:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [31][ 5/10]  base_lr: 1.0000e-02 lr: 2.8225e-03  eta: 0:02:06  time: 1.3233  data_time: 1.2676  memory: 1226  loss: 0.4756  loss_cls: 0.0000  loss_obj: 0.2099  loss_bbox: 0.2657\n",
            "01/24 02:49:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:49:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [31][10/10]  base_lr: 1.0000e-02 lr: 2.8225e-03  eta: 0:01:58  time: 1.3034  data_time: 1.2485  memory: 1226  loss: 0.4784  loss_cls: 0.0000  loss_obj: 0.2139  loss_bbox: 0.2645\n",
            "01/24 02:49:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [32][ 5/10]  base_lr: 1.0000e-02 lr: 2.5750e-03  eta: 0:01:52  time: 1.2903  data_time: 1.2368  memory: 1226  loss: 0.4773  loss_cls: 0.0000  loss_obj: 0.2138  loss_bbox: 0.2635\n",
            "01/24 02:49:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:49:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [32][10/10]  base_lr: 1.0000e-02 lr: 2.5750e-03  eta: 0:01:45  time: 1.2871  data_time: 1.2344  memory: 1226  loss: 0.4776  loss_cls: 0.0000  loss_obj: 0.2150  loss_bbox: 0.2626\n",
            "01/24 02:49:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [33][ 5/10]  base_lr: 1.0000e-02 lr: 2.3275e-03  eta: 0:01:39  time: 1.2873  data_time: 1.2323  memory: 1226  loss: 0.4741  loss_cls: 0.0000  loss_obj: 0.2125  loss_bbox: 0.2616\n",
            "01/24 02:49:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:49:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [33][10/10]  base_lr: 1.0000e-02 lr: 2.3275e-03  eta: 0:01:32  time: 1.2927  data_time: 1.2380  memory: 1226  loss: 0.4737  loss_cls: 0.0000  loss_obj: 0.2126  loss_bbox: 0.2611\n",
            "01/24 02:49:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [34][ 5/10]  base_lr: 1.0000e-02 lr: 2.0800e-03  eta: 0:01:26  time: 1.3110  data_time: 1.2550  memory: 1226  loss: 0.4742  loss_cls: 0.0000  loss_obj: 0.2126  loss_bbox: 0.2616\n",
            "01/24 02:49:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:49:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [34][10/10]  base_lr: 1.0000e-02 lr: 2.0800e-03  eta: 0:01:19  time: 1.3105  data_time: 1.2548  memory: 1226  loss: 0.4718  loss_cls: 0.0000  loss_obj: 0.2128  loss_bbox: 0.2590\n",
            "01/24 02:49:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [35][ 5/10]  base_lr: 1.0000e-02 lr: 1.8325e-03  eta: 0:01:12  time: 1.2971  data_time: 1.2401  memory: 1226  loss: 0.4697  loss_cls: 0.0000  loss_obj: 0.2126  loss_bbox: 0.2570\n",
            "01/24 02:49:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:49:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [35][10/10]  base_lr: 1.0000e-02 lr: 1.8325e-03  eta: 0:01:05  time: 1.2966  data_time: 1.2385  memory: 1226  loss: 0.4620  loss_cls: 0.0000  loss_obj: 0.2093  loss_bbox: 0.2527\n",
            "01/24 02:50:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [36][ 5/10]  base_lr: 1.0000e-02 lr: 1.5850e-03  eta: 0:00:59  time: 1.2836  data_time: 1.2261  memory: 1226  loss: 0.4596  loss_cls: 0.0000  loss_obj: 0.2094  loss_bbox: 0.2502\n",
            "01/24 02:50:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:50:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [36][10/10]  base_lr: 1.0000e-02 lr: 1.5850e-03  eta: 0:00:52  time: 1.2848  data_time: 1.2265  memory: 1226  loss: 0.4572  loss_cls: 0.0000  loss_obj: 0.2080  loss_bbox: 0.2492\n",
            "01/24 02:50:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [37][ 5/10]  base_lr: 1.0000e-02 lr: 1.3375e-03  eta: 0:00:46  time: 1.3065  data_time: 1.2476  memory: 1226  loss: 0.4540  loss_cls: 0.0000  loss_obj: 0.2081  loss_bbox: 0.2460\n",
            "01/24 02:50:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:50:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [37][10/10]  base_lr: 1.0000e-02 lr: 1.3375e-03  eta: 0:00:39  time: 1.3113  data_time: 1.2526  memory: 1226  loss: 0.4531  loss_cls: 0.0000  loss_obj: 0.2074  loss_bbox: 0.2457\n",
            "01/24 02:50:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [38][ 5/10]  base_lr: 1.0000e-02 lr: 1.0900e-03  eta: 0:00:33  time: 1.3064  data_time: 1.2498  memory: 1226  loss: 0.4505  loss_cls: 0.0000  loss_obj: 0.2078  loss_bbox: 0.2428\n",
            "01/24 02:50:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:50:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [38][10/10]  base_lr: 1.0000e-02 lr: 1.0900e-03  eta: 0:00:26  time: 1.2922  data_time: 1.2350  memory: 1226  loss: 0.4503  loss_cls: 0.0000  loss_obj: 0.2089  loss_bbox: 0.2414\n",
            "01/24 02:50:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [39][ 5/10]  base_lr: 1.0000e-02 lr: 8.4250e-04  eta: 0:00:19  time: 1.2794  data_time: 1.2235  memory: 1226  loss: 0.4443  loss_cls: 0.0000  loss_obj: 0.2066  loss_bbox: 0.2378\n",
            "01/24 02:50:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:50:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [39][10/10]  base_lr: 1.0000e-02 lr: 8.4250e-04  eta: 0:00:13  time: 1.2809  data_time: 1.2231  memory: 1226  loss: 0.4435  loss_cls: 0.0000  loss_obj: 0.2081  loss_bbox: 0.2353\n",
            "01/24 02:50:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [40][ 5/10]  base_lr: 1.0000e-02 lr: 5.9500e-04  eta: 0:00:06  time: 1.2783  data_time: 1.2207  memory: 1226  loss: 0.4436  loss_cls: 0.0000  loss_obj: 0.2100  loss_bbox: 0.2336\n",
            "01/24 02:51:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: yolov5_s-v61_fast_1xb12-40e_cat_20240124_024153\n",
            "01/24 02:51:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [40][10/10]  base_lr: 1.0000e-02 lr: 5.9500e-04  eta: 0:00:00  time: 1.2975  data_time: 1.2411  memory: 1226  loss: 0.4423  loss_cls: 0.0000  loss_obj: 0.2078  loss_bbox: 0.2345\n",
            "01/24 02:51:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 40 epochs\n",
            "01/24 02:51:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [40][ 5/28]    eta: 0:00:03  time: 0.0878  data_time: 0.0574  memory: 116  \n",
            "01/24 02:51:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [40][10/28]    eta: 0:00:01  time: 0.0847  data_time: 0.0528  memory: 129  \n",
            "01/24 02:51:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [40][15/28]    eta: 0:00:01  time: 0.0830  data_time: 0.0513  memory: 129  \n",
            "01/24 02:51:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [40][20/28]    eta: 0:00:00  time: 0.0796  data_time: 0.0474  memory: 116  \n",
            "01/24 02:51:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [40][25/28]    eta: 0:00:00  time: 0.0734  data_time: 0.0408  memory: 116  \n",
            "01/24 02:51:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.02s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.750\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.961\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.864\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.750\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.690\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.807\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.820\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.820\n",
            "01/24 02:51:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.750 0.961 0.864 -1.000 -1.000 0.750\n",
            "01/24 02:51:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [40][28/28]    coco/bbox_mAP: 0.7500  coco/bbox_mAP_50: 0.9610  coco/bbox_mAP_75: 0.8640  coco/bbox_mAP_s: -1.0000  coco/bbox_mAP_m: -1.0000  coco/bbox_mAP_l: 0.7500  data_time: 0.0432  time: 0.0778\n",
            "01/24 02:51:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The previous best checkpoint /mmyolo/work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/best_coco_bbox_mAP_epoch_30.pth is removed\n",
            "01/24 02:51:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - The best checkpoint with 0.7500 coco/bbox_mAP at 40 epoch is saved to best_coco_bbox_mAP_epoch_40.pth.\n"
          ]
        }
      ],
      "source": [
        "!cd .. && python ./tools/train.py ./configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py\n",
        "\n",
        "# albumentations is not installed\n",
        "# !mim install -r ../requirements/albu.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ0h6sv_rJxq"
      },
      "source": [
        "Run the above training command, `work_dirs/yolov5_s-v61_fast_1xb12-40e_cat` folder will be automatically generated, the checkpoint file and the training config file will be saved in this folder. On a low-end 1660 GPU, the entire training process takes about eight minutes.\n",
        "\n",
        "<div align=center>\n",
        "<img src=\"https://user-images.githubusercontent.com/17425982/220236361-bd113606-248e-4a0e-a484-c0dc9e355b5b.png\" alt=\"image\"/>\n",
        "</div>\n",
        "\n",
        "The performance on `test.json` is as follows:\n",
        "\n",
        "```text\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.631\n",
        " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.909\n",
        " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.747\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.631\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.627\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.703\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.703\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.703\n",
        "```\n",
        "\n",
        "The above properties are printed via the COCO API, where -1 indicates that no object exists for the scale. According to the rules defined by COCO, the Cat dataset contains all large sized objects, and there are no small or medium-sized objects.\n",
        "\n",
        "### Some Notes\n",
        "\n",
        "Two key warnings are printed during training:\n",
        "\n",
        "- You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
        "- The model and loaded state dict do not match exactly\n",
        "\n",
        "Neither of these warnings will have any impact on performance. The first warning is because the `num_classes` currently trained is 1, the loss of the classification branch is always 0 according to the community of the YOLOv5 algorithm, which is a normal phenomenon. The second warning is because we are currently training in fine-tuning mode, we load the COCO pre-trained weights for 80 classes,\n",
        "This will lead to the final Head module convolution channel number does not correspond, resulting in this part of the weight can not be loaded, which is also a normal phenomenon.\n",
        "\n",
        "### Training is resumed after the interruption\n",
        "\n",
        "If you stop training, you can add `--resume` to the end of the training command and the program will automatically resume training with the latest weights file from `work_dirs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python tools/train.py configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py --resume"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3sJxvQoUrMhX"
      },
      "source": [
        "### Save GPU memory strategy\n",
        "\n",
        "The above config requires about 3G RAM, so if you don't have enough, consider turning on mixed-precision training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python tools/train.py configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py --amp"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jVJdyHTxrQ9a"
      },
      "source": [
        "### Training visualization\n",
        "\n",
        "MMYOLO currently supports local, TensorBoard, WandB and other back-end visualization. The default is to use local visualization, and you can switch to WandB and other real-time visualization of various indicators in the training process.\n",
        "\n",
        "#### 1 WandB\n",
        "\n",
        "WandB visualization need registered in website, and in the https://wandb.ai/settings for wandb API Keys.\n",
        "\n",
        "<div align=center>\n",
        "<img src=\"https://cdn.vansin.top/img/20220913212628.png\" alt=\"image\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install wandb\n",
        "# # After running wandb login, enter the API Keys obtained above, and the login is successful.\n",
        "# !wandb login"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu0_4YYRrbyY"
      },
      "source": [
        "Add the wandb config at the end of config file we just created: `configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py`.\n",
        "\n",
        "```python\n",
        "visualizer = dict(vis_backends = [dict(type='LocalVisBackend'), dict(type='WandbVisBackend')])\n",
        "```\n",
        "\n",
        "Running the training command and you will see the loss, learning rate, and coco/bbox_mAP visualizations in the link."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python tools/train.py configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f_DyzfDIzwMa"
      },
      "source": [
        "<div align=center>\n",
        "<img src=\"https://user-images.githubusercontent.com/17425982/222131114-30a79285-56bc-427d-a38d-8d6a6982ad60.png\" alt=\"image\"/>\n",
        "</div>\n",
        "<div align=center>\n",
        "<img src=\"https://user-images.githubusercontent.com/17425982/222132585-4b4962f1-211b-46f7-86b3-7534fc52a1b4.png\" alt=\"image\"/>\n",
        "</div>\n",
        "\n",
        "#### 2 Tensorboard\n",
        "\n",
        "Install Tensorboard using the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "gHkGlii3n29Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Collecting tensorboard\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6f/77/e624b4916531721e674aa105151ffa5223fb224d3ca4bd5c10574664f944/tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.0 MB 852 kB/s eta 0:00:01\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e0/68/e8ecfac5dd594b676c23a7f07ea34c197d7d69b3313afdf8ac1b0a9905a2/tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 781 kB 12.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting absl-py>=0.4\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 133 kB 10.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting grpcio>=1.24.3\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/21/e8/3837e717dc39c552722f2d0e598f2b251054fee5ac81d1ab97a77c1ab3df/grpcio-1.60.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.4 MB 12.6 MB/s eta 0:00:01     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 4.7 MB 12.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/aa/42/c3873f5a4369d28eb0006bfc80837f28b18bd4e04526f55cc9c8eac7a803/google_auth-2.26.2-py2.py3-none-any.whl (186 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 186 kB 12.8 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (1.20.2)\n",
            "Collecting protobuf<4,>=3.9.2\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4c/12/62e1d5505c172e1a7f803d83b0b1693f7952c3c271eb2f155703012ae67a/protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.0 MB 14.2 MB/s eta 0:00:01     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 839 kB 14.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (0.35.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (60.2.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/60/f9/802efd84988bffd9f644c03b6e66fde8e76c3aa33db4279ddd11c5d61f4b/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.9 MB 12.2 MB/s eta 0:00:01     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 1.5 MB 12.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (2.28.2)\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f6/f8/9da63c1617ae2a1dec2fbf6412f3a0cfe9d4ce029eccbda6e1e4258ca45f/Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 233 kB 12.3 MB/s eta 0:00:01     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 204 kB 12.3 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b1/0e/0636cc1448a7abc444fb1b3a63655e294e0d2d49092dc3de05241be6d43c/google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard) (3.4.4)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/cd/8e/bea464350e1b8c6ed0da3a312659cb648804a08af6cacc6435867f74f8bd/pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181 kB 13.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6f/bb/5deac77a9af870143c684ab46a7934038a53eb4aa975bc0687ed6ca2c610/requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard) (6.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (4.7.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.15.0)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d1/75/4686d2872bf2fc0b37917cbc8bbf0dd3a5cdb0990799be1b9cbf1e1eb733/pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84 kB 16.3 MB/s eta 0:00:011\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7e/80/cab10959dc1faead58dc8384a781dfbf93cb4d33d50988f7a69f1b7c9bbe/oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151 kB 12.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting MarkupSafe>=2.1.1\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/19/82/dfa26a17c74a592865fe9f2f1b2c19a103672207ace9a611b35131f5aaf6/MarkupSafe-2.1.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, MarkupSafe, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, grpcio, google-auth-oauthlib, absl-py, tensorboard\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "Successfully installed MarkupSafe-2.1.4 absl-py-2.1.0 cachetools-5.3.2 google-auth-2.26.2 google-auth-oauthlib-0.4.6 grpcio-1.60.0 oauthlib-3.2.2 protobuf-3.20.3 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 werkzeug-2.2.3\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorboard"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bE-nx9TY1P-M"
      },
      "source": [
        "Add the `tensorboard` config at the end of config file we just created: `configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py`.\n",
        "\n",
        "```python\n",
        "visualizer = dict(vis_backends=[dict(type='LocalVisBackend'),dict(type='TensorboardVisBackend')])\n",
        "```\n",
        "\n",
        "After re-running the training command, Tensorboard file will be generated in the visualization folder `work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/{timestamp}/vis_data`.\n",
        "We can use Tensorboard to view the loss, learning rate, and coco/bbox_mAP visualizations from a web link by running the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "id": "g8fZgokho5CE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow installation not found - running with reduced feature set.\n",
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.11.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!cd .. && tensorboard --logdir=work_dirs/yolov5_s-v61_fast_1xb12-40e_cat"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GUZ7MPoaro-o"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYmxtE0GunTB",
        "outputId": "f440807c-1931-4810-b76d-617f73fde227"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "01/24 03:03:30 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmyolo\" in the \"log_processor\" registry tree. As a workaround, the current \"log_processor\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmyolo\" is a correct scope, or whether the registry is initialized.\n",
            "01/24 03:03:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
            "------------------------------------------------------------\n",
            "System environment:\n",
            "    sys.platform: linux\n",
            "    Python: 3.7.10 (default, Feb 26 2021, 18:47:35) [GCC 7.3.0]\n",
            "    CUDA available: True\n",
            "    numpy_random_seed: 1121750788\n",
            "    GPU 0: NVIDIA GeForce RTX 3090\n",
            "    CUDA_HOME: /usr/local/cuda\n",
            "    NVCC: Cuda compilation tools, release 11.1, V11.1.105\n",
            "    GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "    PyTorch: 1.9.0\n",
            "    PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2021.2-Product Build 20210312 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 11.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37\n",
            "  - CuDNN 8.0.5\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "    TorchVision: 0.10.0\n",
            "    OpenCV: 4.9.0\n",
            "    MMEngine: 0.10.2\n",
            "\n",
            "Runtime environment:\n",
            "    cudnn_benchmark: True\n",
            "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
            "    dist_cfg: {'backend': 'nccl'}\n",
            "    seed: 1121750788\n",
            "    Distributed launcher: none\n",
            "    Distributed training: False\n",
            "    GPU number: 1\n",
            "------------------------------------------------------------\n",
            "\n",
            "01/24 03:03:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
            "_backend_args = None\n",
            "_multiscale_resize_transforms = [\n",
            "    dict(\n",
            "        transforms=[\n",
            "            dict(scale=(\n",
            "                640,\n",
            "                640,\n",
            "            ), type='YOLOv5KeepRatioResize'),\n",
            "            dict(\n",
            "                allow_scale_up=False,\n",
            "                pad_val=dict(img=114),\n",
            "                scale=(\n",
            "                    640,\n",
            "                    640,\n",
            "                ),\n",
            "                type='LetterResize'),\n",
            "        ],\n",
            "        type='Compose'),\n",
            "    dict(\n",
            "        transforms=[\n",
            "            dict(scale=(\n",
            "                320,\n",
            "                320,\n",
            "            ), type='YOLOv5KeepRatioResize'),\n",
            "            dict(\n",
            "                allow_scale_up=False,\n",
            "                pad_val=dict(img=114),\n",
            "                scale=(\n",
            "                    320,\n",
            "                    320,\n",
            "                ),\n",
            "                type='LetterResize'),\n",
            "        ],\n",
            "        type='Compose'),\n",
            "    dict(\n",
            "        transforms=[\n",
            "            dict(scale=(\n",
            "                960,\n",
            "                960,\n",
            "            ), type='YOLOv5KeepRatioResize'),\n",
            "            dict(\n",
            "                allow_scale_up=False,\n",
            "                pad_val=dict(img=114),\n",
            "                scale=(\n",
            "                    960,\n",
            "                    960,\n",
            "                ),\n",
            "                type='LetterResize'),\n",
            "        ],\n",
            "        type='Compose'),\n",
            "]\n",
            "affine_scale = 0.5\n",
            "albu_train_transforms = [\n",
            "    dict(p=0.01, type='Blur'),\n",
            "    dict(p=0.01, type='MedianBlur'),\n",
            "    dict(p=0.01, type='ToGray'),\n",
            "    dict(p=0.01, type='CLAHE'),\n",
            "]\n",
            "anchors = [\n",
            "    [\n",
            "        (\n",
            "            68,\n",
            "            69,\n",
            "        ),\n",
            "        (\n",
            "            154,\n",
            "            91,\n",
            "        ),\n",
            "        (\n",
            "            143,\n",
            "            162,\n",
            "        ),\n",
            "    ],\n",
            "    [\n",
            "        (\n",
            "            242,\n",
            "            160,\n",
            "        ),\n",
            "        (\n",
            "            189,\n",
            "            287,\n",
            "        ),\n",
            "        (\n",
            "            391,\n",
            "            207,\n",
            "        ),\n",
            "    ],\n",
            "    [\n",
            "        (\n",
            "            353,\n",
            "            337,\n",
            "        ),\n",
            "        (\n",
            "            539,\n",
            "            341,\n",
            "        ),\n",
            "        (\n",
            "            443,\n",
            "            432,\n",
            "        ),\n",
            "    ],\n",
            "]\n",
            "backend_args = None\n",
            "base_lr = 0.01\n",
            "batch_shapes_cfg = dict(\n",
            "    batch_size=1,\n",
            "    extra_pad_ratio=0.5,\n",
            "    img_size=640,\n",
            "    size_divisor=32,\n",
            "    type='BatchShapePolicy')\n",
            "class_name = ('cat', )\n",
            "custom_hooks = [\n",
            "    dict(\n",
            "        ema_type='ExpMomentumEMA',\n",
            "        momentum=0.0001,\n",
            "        priority=49,\n",
            "        strict_load=False,\n",
            "        type='EMAHook',\n",
            "        update_buffers=True),\n",
            "]\n",
            "data_root = './data/cat/'\n",
            "dataset_type = 'YOLOv5CocoDataset'\n",
            "deepen_factor = 0.33\n",
            "default_hooks = dict(\n",
            "    checkpoint=dict(\n",
            "        interval=10, max_keep_ckpts=2, save_best='auto',\n",
            "        type='CheckpointHook'),\n",
            "    logger=dict(interval=5, type='LoggerHook'),\n",
            "    param_scheduler=dict(\n",
            "        lr_factor=0.01,\n",
            "        max_epochs=40,\n",
            "        scheduler_type='linear',\n",
            "        type='YOLOv5ParamSchedulerHook',\n",
            "        warmup_mim_iter=10),\n",
            "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
            "    timer=dict(type='IterTimerHook'),\n",
            "    visualization=dict(\n",
            "        draw=True,\n",
            "        test_out_dir='show_results',\n",
            "        type='mmdet.DetVisualizationHook'))\n",
            "default_scope = 'mmyolo'\n",
            "env_cfg = dict(\n",
            "    cudnn_benchmark=True,\n",
            "    dist_cfg=dict(backend='nccl'),\n",
            "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
            "img_scale = (\n",
            "    640,\n",
            "    640,\n",
            ")\n",
            "img_scales = [\n",
            "    (\n",
            "        640,\n",
            "        640,\n",
            "    ),\n",
            "    (\n",
            "        320,\n",
            "        320,\n",
            "    ),\n",
            "    (\n",
            "        960,\n",
            "        960,\n",
            "    ),\n",
            "]\n",
            "launcher = 'none'\n",
            "load_from = 'work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth'\n",
            "log_level = 'INFO'\n",
            "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
            "loss_bbox_weight = 0.05\n",
            "loss_cls_weight = 0.5\n",
            "loss_obj_weight = 1.0\n",
            "lr_factor = 0.01\n",
            "max_epochs = 40\n",
            "max_keep_ckpts = 3\n",
            "metainfo = dict(\n",
            "    classes=('cat', ), palette=[\n",
            "        (\n",
            "            20,\n",
            "            220,\n",
            "            60,\n",
            "        ),\n",
            "    ])\n",
            "model = dict(\n",
            "    backbone=dict(\n",
            "        act_cfg=dict(inplace=True, type='SiLU'),\n",
            "        deepen_factor=0.33,\n",
            "        frozen_stages=4,\n",
            "        norm_cfg=dict(eps=0.001, momentum=0.03, type='BN'),\n",
            "        type='YOLOv5CSPDarknet',\n",
            "        widen_factor=0.5),\n",
            "    bbox_head=dict(\n",
            "        head_module=dict(\n",
            "            featmap_strides=[\n",
            "                8,\n",
            "                16,\n",
            "                32,\n",
            "            ],\n",
            "            in_channels=[\n",
            "                256,\n",
            "                512,\n",
            "                1024,\n",
            "            ],\n",
            "            num_base_priors=3,\n",
            "            num_classes=1,\n",
            "            type='YOLOv5HeadModule',\n",
            "            widen_factor=0.5),\n",
            "        loss_bbox=dict(\n",
            "            bbox_format='xywh',\n",
            "            eps=1e-07,\n",
            "            iou_mode='ciou',\n",
            "            loss_weight=0.05,\n",
            "            reduction='mean',\n",
            "            return_iou=True,\n",
            "            type='IoULoss'),\n",
            "        loss_cls=dict(\n",
            "            loss_weight=0.5,\n",
            "            reduction='mean',\n",
            "            type='mmdet.CrossEntropyLoss',\n",
            "            use_sigmoid=True),\n",
            "        loss_obj=dict(\n",
            "            loss_weight=1.0,\n",
            "            reduction='mean',\n",
            "            type='mmdet.CrossEntropyLoss',\n",
            "            use_sigmoid=True),\n",
            "        obj_level_weights=[\n",
            "            4.0,\n",
            "            1.0,\n",
            "            0.4,\n",
            "        ],\n",
            "        prior_generator=dict(\n",
            "            base_sizes=[\n",
            "                [\n",
            "                    (\n",
            "                        68,\n",
            "                        69,\n",
            "                    ),\n",
            "                    (\n",
            "                        154,\n",
            "                        91,\n",
            "                    ),\n",
            "                    (\n",
            "                        143,\n",
            "                        162,\n",
            "                    ),\n",
            "                ],\n",
            "                [\n",
            "                    (\n",
            "                        242,\n",
            "                        160,\n",
            "                    ),\n",
            "                    (\n",
            "                        189,\n",
            "                        287,\n",
            "                    ),\n",
            "                    (\n",
            "                        391,\n",
            "                        207,\n",
            "                    ),\n",
            "                ],\n",
            "                [\n",
            "                    (\n",
            "                        353,\n",
            "                        337,\n",
            "                    ),\n",
            "                    (\n",
            "                        539,\n",
            "                        341,\n",
            "                    ),\n",
            "                    (\n",
            "                        443,\n",
            "                        432,\n",
            "                    ),\n",
            "                ],\n",
            "            ],\n",
            "            strides=[\n",
            "                8,\n",
            "                16,\n",
            "                32,\n",
            "            ],\n",
            "            type='mmdet.YOLOAnchorGenerator'),\n",
            "        prior_match_thr=4.0,\n",
            "        type='YOLOv5Head'),\n",
            "    data_preprocessor=dict(\n",
            "        bgr_to_rgb=True,\n",
            "        mean=[\n",
            "            0.0,\n",
            "            0.0,\n",
            "            0.0,\n",
            "        ],\n",
            "        std=[\n",
            "            255.0,\n",
            "            255.0,\n",
            "            255.0,\n",
            "        ],\n",
            "        type='YOLOv5DetDataPreprocessor'),\n",
            "    neck=dict(\n",
            "        act_cfg=dict(inplace=True, type='SiLU'),\n",
            "        deepen_factor=0.33,\n",
            "        in_channels=[\n",
            "            256,\n",
            "            512,\n",
            "            1024,\n",
            "        ],\n",
            "        norm_cfg=dict(eps=0.001, momentum=0.03, type='BN'),\n",
            "        num_csp_blocks=3,\n",
            "        out_channels=[\n",
            "            256,\n",
            "            512,\n",
            "            1024,\n",
            "        ],\n",
            "        type='YOLOv5PAFPN',\n",
            "        widen_factor=0.5),\n",
            "    test_cfg=dict(\n",
            "        max_per_img=300,\n",
            "        multi_label=True,\n",
            "        nms=dict(iou_threshold=0.65, type='nms'),\n",
            "        nms_pre=30000,\n",
            "        score_thr=0.001),\n",
            "    type='YOLODetector')\n",
            "model_test_cfg = dict(\n",
            "    max_per_img=300,\n",
            "    multi_label=True,\n",
            "    nms=dict(iou_threshold=0.65, type='nms'),\n",
            "    nms_pre=30000,\n",
            "    score_thr=0.001)\n",
            "norm_cfg = dict(eps=0.001, momentum=0.03, type='BN')\n",
            "num_classes = 1\n",
            "num_det_layers = 3\n",
            "obj_level_weights = [\n",
            "    4.0,\n",
            "    1.0,\n",
            "    0.4,\n",
            "]\n",
            "optim_wrapper = dict(\n",
            "    constructor='YOLOv5OptimizerConstructor',\n",
            "    optimizer=dict(\n",
            "        batch_size_per_gpu=12,\n",
            "        lr=0.01,\n",
            "        momentum=0.937,\n",
            "        nesterov=True,\n",
            "        type='SGD',\n",
            "        weight_decay=0.0005),\n",
            "    type='OptimWrapper')\n",
            "param_scheduler = None\n",
            "persistent_workers = True\n",
            "pre_transform = [\n",
            "    dict(backend_args=None, type='LoadImageFromFile'),\n",
            "    dict(type='LoadAnnotations', with_bbox=True),\n",
            "]\n",
            "prior_match_thr = 4.0\n",
            "resume = False\n",
            "save_checkpoint_intervals = 10\n",
            "strides = [\n",
            "    8,\n",
            "    16,\n",
            "    32,\n",
            "]\n",
            "test_cfg = dict(type='TestLoop')\n",
            "test_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='annotations/test.json',\n",
            "        batch_shapes_cfg=dict(\n",
            "            batch_size=1,\n",
            "            extra_pad_ratio=0.5,\n",
            "            img_size=640,\n",
            "            size_divisor=32,\n",
            "            type='BatchShapePolicy'),\n",
            "        data_prefix=dict(img='images/'),\n",
            "        data_root='./data/cat/',\n",
            "        metainfo=dict(classes=('cat', ), palette=[\n",
            "            (\n",
            "                20,\n",
            "                220,\n",
            "                60,\n",
            "            ),\n",
            "        ]),\n",
            "        pipeline=[\n",
            "            dict(backend_args=None, type='LoadImageFromFile'),\n",
            "            dict(scale=(\n",
            "                640,\n",
            "                640,\n",
            "            ), type='YOLOv5KeepRatioResize'),\n",
            "            dict(\n",
            "                allow_scale_up=False,\n",
            "                pad_val=dict(img=114),\n",
            "                scale=(\n",
            "                    640,\n",
            "                    640,\n",
            "                ),\n",
            "                type='LetterResize'),\n",
            "            dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
            "            dict(\n",
            "                meta_keys=(\n",
            "                    'img_id',\n",
            "                    'img_path',\n",
            "                    'ori_shape',\n",
            "                    'img_shape',\n",
            "                    'scale_factor',\n",
            "                    'pad_param',\n",
            "                ),\n",
            "                type='mmdet.PackDetInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='YOLOv5CocoDataset'),\n",
            "    drop_last=False,\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    pin_memory=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "test_evaluator = dict(\n",
            "    ann_file='./data/cat/annotations/test.json',\n",
            "    metric='bbox',\n",
            "    proposal_nums=(\n",
            "        100,\n",
            "        1,\n",
            "        10,\n",
            "    ),\n",
            "    type='mmdet.CocoMetric')\n",
            "test_pipeline = [\n",
            "    dict(backend_args=None, type='LoadImageFromFile'),\n",
            "    dict(scale=(\n",
            "        640,\n",
            "        640,\n",
            "    ), type='YOLOv5KeepRatioResize'),\n",
            "    dict(\n",
            "        allow_scale_up=False,\n",
            "        pad_val=dict(img=114),\n",
            "        scale=(\n",
            "            640,\n",
            "            640,\n",
            "        ),\n",
            "        type='LetterResize'),\n",
            "    dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
            "    dict(\n",
            "        meta_keys=(\n",
            "            'img_id',\n",
            "            'img_path',\n",
            "            'ori_shape',\n",
            "            'img_shape',\n",
            "            'scale_factor',\n",
            "            'pad_param',\n",
            "        ),\n",
            "        type='mmdet.PackDetInputs'),\n",
            "]\n",
            "train_ann_file = 'annotations/instances_train2017.json'\n",
            "train_batch_size_per_gpu = 12\n",
            "train_cfg = dict(max_epochs=40, type='EpochBasedTrainLoop', val_interval=10)\n",
            "train_data_prefix = 'train2017/'\n",
            "train_dataloader = dict(\n",
            "    batch_size=12,\n",
            "    collate_fn=dict(type='yolov5_collate'),\n",
            "    dataset=dict(\n",
            "        ann_file='annotations/trainval.json',\n",
            "        data_prefix=dict(img='images/'),\n",
            "        data_root='./data/cat/',\n",
            "        filter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
            "        metainfo=dict(classes=('cat', ), palette=[\n",
            "            (\n",
            "                20,\n",
            "                220,\n",
            "                60,\n",
            "            ),\n",
            "        ]),\n",
            "        pipeline=[\n",
            "            dict(backend_args=None, type='LoadImageFromFile'),\n",
            "            dict(type='LoadAnnotations', with_bbox=True),\n",
            "            dict(\n",
            "                img_scale=(\n",
            "                    640,\n",
            "                    640,\n",
            "                ),\n",
            "                pad_val=114.0,\n",
            "                pre_transform=[\n",
            "                    dict(backend_args=None, type='LoadImageFromFile'),\n",
            "                    dict(type='LoadAnnotations', with_bbox=True),\n",
            "                ],\n",
            "                type='Mosaic'),\n",
            "            dict(\n",
            "                border=(\n",
            "                    -320,\n",
            "                    -320,\n",
            "                ),\n",
            "                border_val=(\n",
            "                    114,\n",
            "                    114,\n",
            "                    114,\n",
            "                ),\n",
            "                max_rotate_degree=0.0,\n",
            "                max_shear_degree=0.0,\n",
            "                scaling_ratio_range=(\n",
            "                    0.5,\n",
            "                    1.5,\n",
            "                ),\n",
            "                type='YOLOv5RandomAffine'),\n",
            "            dict(\n",
            "                bbox_params=dict(\n",
            "                    format='pascal_voc',\n",
            "                    label_fields=[\n",
            "                        'gt_bboxes_labels',\n",
            "                        'gt_ignore_flags',\n",
            "                    ],\n",
            "                    type='BboxParams'),\n",
            "                keymap=dict(gt_bboxes='bboxes', img='image'),\n",
            "                transforms=[\n",
            "                    dict(p=0.01, type='Blur'),\n",
            "                    dict(p=0.01, type='MedianBlur'),\n",
            "                    dict(p=0.01, type='ToGray'),\n",
            "                    dict(p=0.01, type='CLAHE'),\n",
            "                ],\n",
            "                type='mmdet.Albu'),\n",
            "            dict(type='YOLOv5HSVRandomAug'),\n",
            "            dict(prob=0.5, type='mmdet.RandomFlip'),\n",
            "            dict(\n",
            "                meta_keys=(\n",
            "                    'img_id',\n",
            "                    'img_path',\n",
            "                    'ori_shape',\n",
            "                    'img_shape',\n",
            "                    'flip',\n",
            "                    'flip_direction',\n",
            "                ),\n",
            "                type='mmdet.PackDetInputs'),\n",
            "        ],\n",
            "        type='YOLOv5CocoDataset'),\n",
            "    num_workers=4,\n",
            "    persistent_workers=True,\n",
            "    pin_memory=True,\n",
            "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
            "train_num_workers = 4\n",
            "train_pipeline = [\n",
            "    dict(backend_args=None, type='LoadImageFromFile'),\n",
            "    dict(type='LoadAnnotations', with_bbox=True),\n",
            "    dict(\n",
            "        img_scale=(\n",
            "            640,\n",
            "            640,\n",
            "        ),\n",
            "        pad_val=114.0,\n",
            "        pre_transform=[\n",
            "            dict(backend_args=None, type='LoadImageFromFile'),\n",
            "            dict(type='LoadAnnotations', with_bbox=True),\n",
            "        ],\n",
            "        type='Mosaic'),\n",
            "    dict(\n",
            "        border=(\n",
            "            -320,\n",
            "            -320,\n",
            "        ),\n",
            "        border_val=(\n",
            "            114,\n",
            "            114,\n",
            "            114,\n",
            "        ),\n",
            "        max_rotate_degree=0.0,\n",
            "        max_shear_degree=0.0,\n",
            "        scaling_ratio_range=(\n",
            "            0.5,\n",
            "            1.5,\n",
            "        ),\n",
            "        type='YOLOv5RandomAffine'),\n",
            "    dict(\n",
            "        bbox_params=dict(\n",
            "            format='pascal_voc',\n",
            "            label_fields=[\n",
            "                'gt_bboxes_labels',\n",
            "                'gt_ignore_flags',\n",
            "            ],\n",
            "            type='BboxParams'),\n",
            "        keymap=dict(gt_bboxes='bboxes', img='image'),\n",
            "        transforms=[\n",
            "            dict(p=0.01, type='Blur'),\n",
            "            dict(p=0.01, type='MedianBlur'),\n",
            "            dict(p=0.01, type='ToGray'),\n",
            "            dict(p=0.01, type='CLAHE'),\n",
            "        ],\n",
            "        type='mmdet.Albu'),\n",
            "    dict(type='YOLOv5HSVRandomAug'),\n",
            "    dict(prob=0.5, type='mmdet.RandomFlip'),\n",
            "    dict(\n",
            "        meta_keys=(\n",
            "            'img_id',\n",
            "            'img_path',\n",
            "            'ori_shape',\n",
            "            'img_shape',\n",
            "            'flip',\n",
            "            'flip_direction',\n",
            "        ),\n",
            "        type='mmdet.PackDetInputs'),\n",
            "]\n",
            "tta_model = dict(\n",
            "    tta_cfg=dict(max_per_img=300, nms=dict(iou_threshold=0.65, type='nms')),\n",
            "    type='mmdet.DetTTAModel')\n",
            "tta_pipeline = [\n",
            "    dict(backend_args=None, type='LoadImageFromFile'),\n",
            "    dict(\n",
            "        transforms=[\n",
            "            [\n",
            "                dict(\n",
            "                    transforms=[\n",
            "                        dict(scale=(\n",
            "                            640,\n",
            "                            640,\n",
            "                        ), type='YOLOv5KeepRatioResize'),\n",
            "                        dict(\n",
            "                            allow_scale_up=False,\n",
            "                            pad_val=dict(img=114),\n",
            "                            scale=(\n",
            "                                640,\n",
            "                                640,\n",
            "                            ),\n",
            "                            type='LetterResize'),\n",
            "                    ],\n",
            "                    type='Compose'),\n",
            "                dict(\n",
            "                    transforms=[\n",
            "                        dict(scale=(\n",
            "                            320,\n",
            "                            320,\n",
            "                        ), type='YOLOv5KeepRatioResize'),\n",
            "                        dict(\n",
            "                            allow_scale_up=False,\n",
            "                            pad_val=dict(img=114),\n",
            "                            scale=(\n",
            "                                320,\n",
            "                                320,\n",
            "                            ),\n",
            "                            type='LetterResize'),\n",
            "                    ],\n",
            "                    type='Compose'),\n",
            "                dict(\n",
            "                    transforms=[\n",
            "                        dict(scale=(\n",
            "                            960,\n",
            "                            960,\n",
            "                        ), type='YOLOv5KeepRatioResize'),\n",
            "                        dict(\n",
            "                            allow_scale_up=False,\n",
            "                            pad_val=dict(img=114),\n",
            "                            scale=(\n",
            "                                960,\n",
            "                                960,\n",
            "                            ),\n",
            "                            type='LetterResize'),\n",
            "                    ],\n",
            "                    type='Compose'),\n",
            "            ],\n",
            "            [\n",
            "                dict(prob=1.0, type='mmdet.RandomFlip'),\n",
            "                dict(prob=0.0, type='mmdet.RandomFlip'),\n",
            "            ],\n",
            "            [\n",
            "                dict(type='mmdet.LoadAnnotations', with_bbox=True),\n",
            "            ],\n",
            "            [\n",
            "                dict(\n",
            "                    meta_keys=(\n",
            "                        'img_id',\n",
            "                        'img_path',\n",
            "                        'ori_shape',\n",
            "                        'img_shape',\n",
            "                        'scale_factor',\n",
            "                        'pad_param',\n",
            "                        'flip',\n",
            "                        'flip_direction',\n",
            "                    ),\n",
            "                    type='mmdet.PackDetInputs'),\n",
            "            ],\n",
            "        ],\n",
            "        type='TestTimeAug'),\n",
            "]\n",
            "val_ann_file = 'annotations/instances_val2017.json'\n",
            "val_batch_size_per_gpu = 1\n",
            "val_cfg = dict(type='ValLoop')\n",
            "val_data_prefix = 'val2017/'\n",
            "val_dataloader = dict(\n",
            "    batch_size=1,\n",
            "    dataset=dict(\n",
            "        ann_file='annotations/test.json',\n",
            "        batch_shapes_cfg=dict(\n",
            "            batch_size=1,\n",
            "            extra_pad_ratio=0.5,\n",
            "            img_size=640,\n",
            "            size_divisor=32,\n",
            "            type='BatchShapePolicy'),\n",
            "        data_prefix=dict(img='images/'),\n",
            "        data_root='./data/cat/',\n",
            "        metainfo=dict(classes=('cat', ), palette=[\n",
            "            (\n",
            "                20,\n",
            "                220,\n",
            "                60,\n",
            "            ),\n",
            "        ]),\n",
            "        pipeline=[\n",
            "            dict(backend_args=None, type='LoadImageFromFile'),\n",
            "            dict(scale=(\n",
            "                640,\n",
            "                640,\n",
            "            ), type='YOLOv5KeepRatioResize'),\n",
            "            dict(\n",
            "                allow_scale_up=False,\n",
            "                pad_val=dict(img=114),\n",
            "                scale=(\n",
            "                    640,\n",
            "                    640,\n",
            "                ),\n",
            "                type='LetterResize'),\n",
            "            dict(_scope_='mmdet', type='LoadAnnotations', with_bbox=True),\n",
            "            dict(\n",
            "                meta_keys=(\n",
            "                    'img_id',\n",
            "                    'img_path',\n",
            "                    'ori_shape',\n",
            "                    'img_shape',\n",
            "                    'scale_factor',\n",
            "                    'pad_param',\n",
            "                ),\n",
            "                type='mmdet.PackDetInputs'),\n",
            "        ],\n",
            "        test_mode=True,\n",
            "        type='YOLOv5CocoDataset'),\n",
            "    drop_last=False,\n",
            "    num_workers=2,\n",
            "    persistent_workers=True,\n",
            "    pin_memory=True,\n",
            "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
            "val_evaluator = dict(\n",
            "    ann_file='./data/cat/annotations/test.json',\n",
            "    metric='bbox',\n",
            "    proposal_nums=(\n",
            "        100,\n",
            "        1,\n",
            "        10,\n",
            "    ),\n",
            "    type='mmdet.CocoMetric')\n",
            "val_num_workers = 2\n",
            "vis_backends = [\n",
            "    dict(type='LocalVisBackend'),\n",
            "]\n",
            "visualizer = dict(\n",
            "    name='visualizer',\n",
            "    type='mmdet.DetLocalVisualizer',\n",
            "    vis_backends=[\n",
            "        dict(type='LocalVisBackend'),\n",
            "    ])\n",
            "weight_decay = 0.0005\n",
            "widen_factor = 0.5\n",
            "work_dir = './work_dirs/yolov5_s-v61_fast_1xb12-40e_cat'\n",
            "\n",
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "01/24 03:03:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
            "01/24 03:03:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(49          ) EMAHook                            \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_load_checkpoint:\n",
            "(49          ) EMAHook                            \n",
            " -------------------- \n",
            "before_train:\n",
            "(9           ) YOLOv5ParamSchedulerHook           \n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(49          ) EMAHook                            \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DistSamplerSeedHook                \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(9           ) YOLOv5ParamSchedulerHook           \n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(9           ) YOLOv5ParamSchedulerHook           \n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(49          ) EMAHook                            \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(9           ) YOLOv5ParamSchedulerHook           \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(49          ) EMAHook                            \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DetVisualizationHook               \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(9           ) YOLOv5ParamSchedulerHook           \n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(49          ) EMAHook                            \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "after_val:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_save_checkpoint:\n",
            "(49          ) EMAHook                            \n",
            " -------------------- \n",
            "after_train:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(VERY_LOW    ) CheckpointHook                     \n",
            " -------------------- \n",
            "before_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "before_test_epoch:\n",
            "(49          ) EMAHook                            \n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "before_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_test_iter:\n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(NORMAL      ) DetVisualizationHook               \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test_epoch:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            "(49          ) EMAHook                            \n",
            "(NORMAL      ) IterTimerHook                      \n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "after_test:\n",
            "(VERY_HIGH   ) RuntimeInfoHook                    \n",
            " -------------------- \n",
            "after_run:\n",
            "(BELOW_NORMAL) LoggerHook                         \n",
            " -------------------- \n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loads checkpoint by local backend from path: work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth\n",
            "01/24 03:03:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth\n",
            "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "01/24 03:03:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [ 5/28]    eta: 0:01:05  time: 2.8333  data_time: 2.5565  memory: 4428  \n",
            "01/24 03:04:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [10/28]    eta: 0:00:37  time: 2.0840  data_time: 1.8658  memory: 4431  \n",
            "01/24 03:04:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [15/28]    eta: 0:00:24  time: 1.8919  data_time: 1.7002  memory: 4428  \n",
            "01/24 03:04:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [20/28]    eta: 0:00:14  time: 1.7982  data_time: 1.6424  memory: 94  \n",
            "01/24 03:04:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [25/28]    eta: 0:00:05  time: 1.7245  data_time: 1.5926  memory: 94  \n",
            "/opt/conda/lib/python3.7/site-packages/mmengine/visualization/visualizer.py:762: UserWarning: Warning: The bbox is out of bounds, the drawn bbox may not be in the image\n",
            "  ' the drawn bbox may not be in the image', UserWarning)\n",
            "/opt/conda/lib/python3.7/site-packages/mmengine/visualization/visualizer.py:833: UserWarning: Warning: The polygon is out of bounds, the drawn polygon may not be in the image\n",
            "  ' the drawn polygon may not be in the image', UserWarning)\n",
            "01/24 03:04:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.04s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.00s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.750\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.961\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.864\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.750\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.690\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.807\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.820\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.820\n",
            "01/24 03:04:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.750 0.961 0.864 -1.000 -1.000 0.750\n",
            "01/24 03:04:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [28/28]    coco/bbox_mAP: 0.7500  coco/bbox_mAP_50: 0.9610  coco/bbox_mAP_75: 0.8640  coco/bbox_mAP_s: -1.0000  coco/bbox_mAP_m: -1.0000  coco/bbox_mAP_l: 0.7500  data_time: 1.5659  time: 1.6877\n"
          ]
        }
      ],
      "source": [
        "!cd ..&& python tools/test.py configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "                      work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth \\\n",
        "                      --show-dir show_results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_cFocUqN0BCb"
      },
      "source": [
        "Run the above test command, you can not only get the AP performance printed in the **Training** section, You can also automatically save the result images to the `work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/{timestamp}/show_results` folder. Below is one of the result images, the left image is the actual annotation, and the right image is the inference result of the model.\n",
        "\n",
        "<div align=center>\n",
        "<img src=\"https://user-images.githubusercontent.com/17425982/220251677-6c7e5c8f-9417-4803-97fc-a968d0172ab7.png\" alt=\"result_img\"/>\n",
        "</div>\n",
        "\n",
        "You can also visualize model inference results in a browser window if you use 'WandbVisBackend' or 'TensorboardVisBackend'.\n",
        "\n",
        "## Feature map visualization\n",
        "\n",
        "MMYOLO provides visualization scripts for feature map to analyze the current model training. Please refer to [Feature Map Visualization](../recommended_topics/visualization.md)\n",
        "\n",
        "Due to the bias of direct visualization of `test_pipeline`, we need modify the `test_pipeline` of `configs/yolov5/yolov5_s-v61_syncbn_8xb16-300e_coco.py`,\n",
        "\n",
        "```python\n",
        "test_pipeline = [\n",
        "    dict(\n",
        "        type='LoadImageFromFile',\n",
        "        file_client_args=_base_.file_client_args),\n",
        "    dict(type='YOLOv5KeepRatioResize', scale=img_scale),\n",
        "    dict(\n",
        "        type='LetterResize',\n",
        "        scale=img_scale,\n",
        "        allow_scale_up=False,\n",
        "        pad_val=dict(img=114)),\n",
        "    dict(type='LoadAnnotations', with_bbox=True, _scope_='mmdet'),\n",
        "    dict(\n",
        "        type='mmdet.PackDetInputs',\n",
        "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n",
        "                   'scale_factor', 'pad_param'))\n",
        "]\n",
        "```\n",
        "\n",
        "to the following config:\n",
        "\n",
        "```python\n",
        "test_pipeline = [\n",
        "    dict(\n",
        "        type='LoadImageFromFile',\n",
        "        file_client_args=_base_.file_client_args),\n",
        "    dict(type='mmdet.Resize', scale=img_scale, keep_ratio=False), # modify the LetterResize to mmdet.Resize\n",
        "    dict(type='LoadAnnotations', with_bbox=True, _scope_='mmdet'),\n",
        "    dict(\n",
        "        type='mmdet.PackDetInputs',\n",
        "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n",
        "                   'scale_factor'))\n",
        "]\n",
        "```\n",
        "\n",
        "Let's choose the `data/cat/images/IMG_20221020_112705.jpg` image as an example to visualize the output feature maps of YOLOv5 backbone and neck layers.\n",
        "\n",
        "**1. Visualize the three channels of YOLOv5 backbone**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "Loads checkpoint by local backend from path: work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth\n",
            "/opt/conda/lib/python3.7/site-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
            "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n",
            "[                                                  ] 0/1, elapsed: 0s, ETA:/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "/opt/conda/lib/python3.7/site-packages/mmengine/visualization/visualizer.py:988: UserWarning: Since the spatial dimensions of overlaid_image: (3000, 4000) and featmap: torch.Size([80, 80]) are not same, the feature map will be interpolated. This may cause mismatch problems ÔºÅ\n",
            "  f'Since the spatial dimensions of '\n",
            "/opt/conda/lib/python3.7/site-packages/mmengine/visualization/visualizer.py:988: UserWarning: Since the spatial dimensions of overlaid_image: (3000, 4000) and featmap: torch.Size([40, 40]) are not same, the feature map will be interpolated. This may cause mismatch problems ÔºÅ\n",
            "  f'Since the spatial dimensions of '\n",
            "/opt/conda/lib/python3.7/site-packages/mmengine/visualization/visualizer.py:988: UserWarning: Since the spatial dimensions of overlaid_image: (3000, 4000) and featmap: torch.Size([20, 20]) are not same, the feature map will be interpolated. This may cause mismatch problems ÔºÅ\n",
            "  f'Since the spatial dimensions of '\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1/1, 0.1 task/s, elapsed: 7s, ETA:     0sAll done!\n",
            "Results have been saved at /mmyolo/output\n"
          ]
        }
      ],
      "source": [
        "!cd .. && python demo/featmap_vis_demo.py data/cat/images/IMG_20221020_112705.jpg \\\n",
        "                                 configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "                                 work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth \\\n",
        "                                 --target-layers backbone \\\n",
        "                                 --channel-reduction squeeze_mean"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=center>\n",
        "<img src=\"https://user-images.githubusercontent.com/17425982/220292217-b343a6f4-0c88-4fdb-9680-35d0ff8e5bdb.png\" width=\"800\" alt=\"image\"/>\n",
        "</div>\n",
        "\n",
        "The result will be saved to the output folder in current path. Three output feature maps plotted in the above figure correspond to small, medium and large output feature maps. As the backbone of this training is not actually involved in training, it can be seen from the above figure that the big object cat is predicted on the small feature map, which is in line with the idea of hierarchical detection of object detection.\n",
        "\n",
        "**2. Visualize the three channels of YOLOv5 neck**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "Loads checkpoint by local backend from path: work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth\n",
            "/opt/conda/lib/python3.7/site-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
            "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n",
            "[                                                  ] 0/1, elapsed: 0s, ETA:/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "/opt/conda/lib/python3.7/site-packages/mmengine/visualization/visualizer.py:988: UserWarning: Since the spatial dimensions of overlaid_image: (3000, 4000) and featmap: torch.Size([80, 80]) are not same, the feature map will be interpolated. This may cause mismatch problems ÔºÅ\n",
            "  f'Since the spatial dimensions of '\n",
            "/opt/conda/lib/python3.7/site-packages/mmengine/visualization/visualizer.py:988: UserWarning: Since the spatial dimensions of overlaid_image: (3000, 4000) and featmap: torch.Size([40, 40]) are not same, the feature map will be interpolated. This may cause mismatch problems ÔºÅ\n",
            "  f'Since the spatial dimensions of '\n",
            "/opt/conda/lib/python3.7/site-packages/mmengine/visualization/visualizer.py:988: UserWarning: Since the spatial dimensions of overlaid_image: (3000, 4000) and featmap: torch.Size([20, 20]) are not same, the feature map will be interpolated. This may cause mismatch problems ÔºÅ\n",
            "  f'Since the spatial dimensions of '\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1/1, 0.1 task/s, elapsed: 9s, ETA:     0sAll done!\n",
            "Results have been saved at /mmyolo/output\n"
          ]
        }
      ],
      "source": [
        "!cd .. && python demo/featmap_vis_demo.py data/cat/images/IMG_20221020_112705.jpg \\\n",
        "                                 configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "                                 work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth \\\n",
        "                                 --target-layers neck \\\n",
        "                                 --channel-reduction squeeze_mean"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=center>\n",
        "<img src=\"https://user-images.githubusercontent.com/17425982/220293382-0a241415-e717-4688-a718-5f6d5c844785.png\" width=\"800\" alt=\"image\"/>\n",
        "</div>\n",
        "\n",
        "As can be seen from the above figure, because neck is involved in training, and we also reset anchor, the three output feature maps are forced to simulate the same scale object, resulting in the three output maps of neck are similar, which destroys the original pre-training distribution of backbone. At the same time, it can also be seen that 40 epochs are not enough to train the above dataset, and the feature maps do not perform well.\n",
        "\n",
        "**3. Grad-Based CAM visualization**\n",
        "\n",
        "Based on the above feature map visualization, we can analyze Grad CAM at the feature layer of bbox level.\n",
        "\n",
        "Install `grad-cam` package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Collecting grad-cam\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4b/7b/e9ba96a243a6a51099668743d1aafb4030272f9a89f59876b7697c874b4d/grad-cam-1.4.8.tar.gz (7.8 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.8 MB 772 kB/s eta 0:00:01     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 1.2 MB 772 kB/s eta 0:00:09     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 7.4 MB 772 kB/s eta 0:00:01\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from grad-cam) (9.5.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from grad-cam) (1.9.0)\n",
            "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from grad-cam) (3.5.3)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from grad-cam) (4.65.2)\n",
            "Collecting ttach\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8d/a3/ee48a184a185c1897c582c72240c2c8a0d0aeb5f8051a71d4e4cd930c52d/ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from grad-cam) (1.0.2)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from grad-cam) (0.10.0)\n",
            "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from grad-cam) (4.9.0.80)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from grad-cam) (1.20.2)\n",
            "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7.1->grad-cam) (4.7.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->grad-cam) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->grad-cam) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->grad-cam) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->grad-cam) (4.38.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->grad-cam) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->grad-cam) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->grad-cam) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->grad-cam) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->grad-cam) (1.3.2)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (PEP 517) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for grad-cam: filename=grad_cam-1.4.8-py3-none-any.whl size=38243 sha256=5972ae013fa7928ce07affe63c04b5a2e5f5928fe895b4ff4be051cdb80ec575\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/e3/7f/559e83b16a345c076f98941c459b2cfa0c373ffebf72548873\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: ttach, grad-cam\n",
            "Successfully installed grad-cam-1.4.8 ttach-0.0.3\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install \"grad-cam\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(a) View Grad CAM of the minimum output feature map of the neck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The algorithm currently used is yolov5\n",
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "Loads checkpoint by local backend from path: work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth\n",
            "[                                                  ] 0/1, elapsed: 0s, ETA:/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1/1, 0.8 task/s, elapsed: 1s, ETA:     0sAll done!\n",
            "Results have been saved at /mmyolo/output\n"
          ]
        }
      ],
      "source": [
        "!cd .. && python demo/boxam_vis_demo.py data/cat/images/IMG_20221020_112705.jpg \\\n",
        "                                configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "                                work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth \\\n",
        "                                --target-layer neck.out_layers[2]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9v-dMkePvHMg"
      },
      "source": [
        "<div align=center>\n",
        "<img src=\"https://user-images.githubusercontent.com/17425982/220298462-b0631f27-2366-4864-915a-a4ee21acd4b9.png\" width=\"800\" alt=\"image\"/>\n",
        "</div>\n",
        "\n",
        "(b) View Grad CAM of the medium output feature map of the neck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9H9u0A-3KAD",
        "outputId": "32ca5a56-052f-4930-f53c-41cc3a9dc619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The algorithm currently used is yolov5\n",
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "Loads checkpoint by local backend from path: work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth\n",
            "[                                                  ] 0/1, elapsed: 0s, ETA:/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1/1, 0.8 task/s, elapsed: 1s, ETA:     0sAll done!\n",
            "Results have been saved at /mmyolo/output\n"
          ]
        }
      ],
      "source": [
        "!cd .. && python demo/boxam_vis_demo.py data/cat/images/IMG_20221020_112705.jpg \\\n",
        "                               configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "                               work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth \\\n",
        "                               --target-layer neck.out_layers[1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(c) View Grad CAM of the maximum output feature map of the neck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrKan1U43uUY",
        "outputId": "690f8414-a76b-4fa6-e600-7cc874ce1914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The algorithm currently used is yolov5\n",
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "Loads checkpoint by local backend from path: work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth\n",
            "[                                                  ] 0/1, elapsed: 0s, ETA:/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1/1, 0.7 task/s, elapsed: 1s, ETA:     0sAll done!\n",
            "Results have been saved at /mmyolo/output\n"
          ]
        }
      ],
      "source": [
        "!cd .. && python demo/boxam_vis_demo.py data/cat/images/IMG_20221020_112705.jpg \\\n",
        "                               configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "                               work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth \\\n",
        "                               --target-layer neck.out_layers[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=center>\n",
        "<img src=\"https://user-images.githubusercontent.com/17425982/220297905-e23369db-d383-48f9-b15e-528a70ec7b23.png\" width=\"800\" alt=\"image\"/>\n",
        "</div>\n",
        "\n",
        "## EasyDeploy deployment\n",
        "\n",
        "Here we'll use MMYOLO's [EasyDeploy](../../../projects/easydeploy/) to demonstrate the transformation deployment and basic inference of model.\n",
        "\n",
        "First you need to follow EasyDeploy's [basic documentation](../../../projects/easydeploy/docs/model_convert.md) controls own equipment installed for each library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Collecting onnx\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f7/18/d23e3fdc4226373377614eec2fca6fd13a23fd47651e9a741e131db05540/onnx-1.14.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.6 MB 10.5 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.7/site-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from onnx) (1.20.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.7/site-packages (from onnx) (4.7.1)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.14.1\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Collecting onnx-simplifier\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9c/ed/35f43c45070ff88a01651f16bd34cf6589f36e7ed1412c1ac7ace641a398/onnx_simplifier-0.4.35-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.2 MB 1.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: rich in /opt/conda/lib/python3.7/site-packages (from onnx-simplifier) (13.7.0)\n",
            "Requirement already satisfied: onnx in /opt/conda/lib/python3.7/site-packages (from onnx-simplifier) (1.14.1)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from onnx->onnx-simplifier) (1.20.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.7/site-packages (from onnx->onnx-simplifier) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.7/site-packages (from onnx->onnx-simplifier) (4.7.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from rich->onnx-simplifier) (2.17.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from rich->onnx-simplifier) (2.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.7/site-packages (from markdown-it-py>=2.2.0->rich->onnx-simplifier) (0.1.2)\n",
            "Installing collected packages: onnx-simplifier\n",
            "Successfully installed onnx-simplifier-0.4.35\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Collecting tensorrt\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/46/08/da496b4f20890b6f717f159bbdda98e9248b560e712aa22cd9bb1daafe15/tensorrt-8.6.1.post1.tar.gz (18 kB)\n",
            "Building wheels for collected packages: tensorrt\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for tensorrt: filename=tensorrt-8.6.1.post1-py2.py3-none-any.whl size=17298 sha256=bf69ad90bc3abbbabcc80ddbe5764a677af9771d846c89f2ee192914d8cfdcae\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/de/84/adb8645e155069ab95c5e6f1c0a17917eb5673ed0b54ad776e\n",
            "Successfully built tensorrt\n",
            "Installing collected packages: tensorrt\n",
            "Successfully installed tensorrt-8.6.1.post1\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install onnx\n",
        "%pip install onnx-simplifier # Install if you want to use simplify\n",
        "%pip install tensorrt        # If you have GPU environment and need to output TensorRT model you need to continue execution"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once installed, you can use the following command to transform and deploy the trained model on the cat dataset with one click. The current ONNX version is 1.13.0 and TensorRT version is 8.5.3.1, so keep the `--opset` value of 11. The remaining parameters need to be adjusted according to the config used. Here we export the CPU version of ONNX with the `--backend` set to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "YsRFEecU5C0w",
        "outputId": "c26011d4-2836-4715-cd6b-68836294db33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Export ONNX with bbox decoder and NMS ...\n",
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "Loads checkpoint by local backend from path: work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth\n",
            "Simplify failure: [ShapeInferenceError] (op_type:Gather, node name: Gather_645): [ShapeInferenceError] Inferred shape and existing shape differ in rank: (2) vs (0)\n",
            "ONNX export success, save into work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.onnx\n"
          ]
        }
      ],
      "source": [
        "# !cd .. && python projects/easydeploy/tools/export.py \\\n",
        "# \t    configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "# \t    work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth \\\n",
        "# \t    --work-dir work_dirs/yolov5_s-v61_fast_1xb12-40e_cat \\\n",
        "#         --img-size 640 640 \\\n",
        "#         --batch 1 \\\n",
        "#         --device cpu \\\n",
        "#         --simplify \\\n",
        "# \t    --opset 11 \\\n",
        "# \t    --backend 1 \\\n",
        "# \t    --pre-topk 1000 \\\n",
        "# \t    --keep-topk 100 \\\n",
        "# \t    --iou-threshold 0.65 \\\n",
        "# \t    --score-threshold 0.25\n",
        "!cd .. && python projects/easydeploy/tools/export_onnx.py \\\n",
        "    configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "    work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth \\\n",
        "    --work-dir work_dirs/yolov5_s-v61_fast_1xb12-40e_cat \\\n",
        "    --img-size 640 640 \\\n",
        "    --batch 1 \\\n",
        "    --device cpu \\\n",
        "    --simplify \\\n",
        "    --opset 11 \\\n",
        "    --backend ONNXRUNTIME \\\n",
        "    --pre-topk 1000 \\\n",
        "    --keep-topk 100 \\\n",
        "    --iou-threshold 0.65 \\\n",
        "    --score-threshold 0.25\n",
        "\n",
        "# ModuleNotFoundError: No module named 'onnxruntime'\n",
        "# !pip install onnxruntime\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q1EY415x3Idx"
      },
      "source": [
        "On success, you will get the converted ONNX model under `work-dir`, which is named `epoch_40.onnx` by default.\n",
        "\n",
        "Let's use `epoch_40.onnx` model to perform a basic image inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[                                                  ] 0/1, elapsed: 0s, ETA:2024-01-24 06:00:29.585452295 [W:onnxruntime:, execution_frame.cc:835 VerifyOutputSizes] Expected shape from model of {1,21} does not match actual shape of {1,2} for output scores\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1/1, 6.0 task/s, elapsed: 0s, ETA:     0s"
          ]
        }
      ],
      "source": [
        "!cd .. && python projects/easydeploy/tools/image-demo.py \\\n",
        "        data/cat/images/IMG_20210728_205312.jpg \\\n",
        "        configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "        work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.onnx \\\n",
        "        --device cpu"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IrjiBa5YwDQM"
      },
      "source": [
        "After successful inference, the result image will be generated in the `output` folder of the default MMYOLO root directory. If you want to see the result without saving it, you can add `--show` to the end of the above command. For convenience, the following is the generated result.\n",
        "\n",
        "<div align=center>\n",
        "<img src=\"https://user-images.githubusercontent.com/7219519/221061210-b91e0b5b-652d-4dfc-8451-86a9a36f7d04.png\" width=\"800\" alt=\"image\"/>\n",
        "</div>\n",
        "\n",
        "Let's go on to convert the engine file for TensorRT, because TensorRT needs to be specific to the current environment and deployment version, so make sure to export the parameters, here we export the TensorRT8 file, the `--backend` is 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "d8zxczqiBLoB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Export ONNX with bbox decoder and NMS ...\n",
            "!!!You are using `YOLOv5Head` with num_classes == 1. The loss_cls will be 0. This is a normal phenomenon.\n",
            "Loads checkpoint by local backend from path: work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth\n",
            "ONNX export success, save into work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.onnx\n"
          ]
        }
      ],
      "source": [
        "!cd .. && python projects/easydeploy/tools/export_onnx.py \\\n",
        "    configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "    work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.pth \\\n",
        "    --work-dir work_dirs/yolov5_s-v61_fast_1xb12-40e_cat \\\n",
        "    --img-size 640 640 \\\n",
        "    --batch 1 \\\n",
        "    --device cuda:0 \\\n",
        "    --simplify \\\n",
        "    --opset 11 \\\n",
        "    --backend TENSORRT8 \\\n",
        "    --pre-topk 1000 \\\n",
        "    --keep-topk 100 \\\n",
        "    --iou-threshold 0.65 \\\n",
        "    --score-threshold 0.25\n",
        "    \n",
        "    # ValueError: '2' is not a valid MMYOLOBackend"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting `epoch_40.onnx` is the ONNX file for the TensorRT8 deployment, which we will use to complete the TensorRT engine transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QFh8rIsX_kVw",
        "outputId": "c5bd6929-03a8-400e-be1e-581f32b23f61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[01/24/2024-07:26:48] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
            "[01/24/2024-07:26:48] [TRT] [W] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n",
            "[01/24/2024-07:26:48] [TRT] [W] builtin_op_importers.cpp:5221: Attribute class_agnostic not found in plugin node! Ensure that the plugin creator has a default value defined or the engine may fail to build.\n",
            "[01/24/2024-07:26:48] [TRT] [W] input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "[01/24/2024-07:26:48] [TRT] [W] output \"num_dets\" with shape(1, 1) DataType.INT32\n",
            "[01/24/2024-07:26:48] [TRT] [W] output \"boxes\" with shape(1, 100, 4) DataType.FLOAT\n",
            "[01/24/2024-07:26:48] [TRT] [W] output \"scores\" with shape(1, 100) DataType.FLOAT\n",
            "[01/24/2024-07:26:48] [TRT] [W] output \"labels\" with shape(1, 100) DataType.INT32\n",
            "[01/24/2024-07:27:38] [TRT] [W] Build tensorrt engine finish.\n",
            "Save in /mmyolo/work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.engine\n"
          ]
        }
      ],
      "source": [
        "!cd .. && python projects/easydeploy/tools/build_engine.py \\\n",
        "        work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.onnx \\\n",
        "        --img-size 640 640 \\\n",
        "        --device cuda:0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Successful execution will generate the `end2end.engine` file under `work-dir`:\n",
        "\n",
        "```shell\n",
        "work_dirs/yolov5_s-v61_fast_1xb12-40e_cat\n",
        "‚îú‚îÄ‚îÄ 202302XX_XXXXXX\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ 202302XX_XXXXXX.log\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ vis_data\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ 202302XX_XXXXXX.json\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ config.py\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ scalars.json\n",
        "‚îú‚îÄ‚îÄ best_coco_bbox_mAP_epoch_40.pth\n",
        "‚îú‚îÄ‚îÄ epoch_40.engine\n",
        "‚îú‚îÄ‚îÄ epoch_40.onnx\n",
        "‚îú‚îÄ‚îÄ epoch_30.pth\n",
        "‚îú‚îÄ‚îÄ epoch_40.pth\n",
        "‚îú‚îÄ‚îÄ last_checkpoint\n",
        "‚îî‚îÄ‚îÄ yolov5_s-v61_fast_1xb12-40e_cat.py\n",
        "```\n",
        "\n",
        "Let's continue use `image-demo.py` for image inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rOqXEi-jAI7Y",
        "outputId": "2a21aaaa-d4ba-498a-f985-2a6a2b8d348f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1/1, 5.6 task/s, elapsed: 0s, ETA:     0s"
          ]
        }
      ],
      "source": [
        "!cd .. && python projects/easydeploy/tools/image-demo.py \\\n",
        "        data/cat/images/IMG_20210728_205312.jpg \\\n",
        "        configs/yolov5/yolov5_s-v61_fast_1xb12-40e_cat.py \\\n",
        "        work_dirs/yolov5_s-v61_fast_1xb12-40e_cat/epoch_40.engine \\\n",
        "        --device cuda:0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ocHGUUEA_TjI"
      },
      "source": [
        "<div align=center>\n",
        "<img src=\"https://user-images.githubusercontent.com/7219519/221061291-e7490bb6-5f0c-45ab-9fc4-caf2b62419d6.png\" width=\"800\" alt=\"image\"/>\n",
        "</div>\n",
        "\n",
        "This completes the transformation deployment of the trained model and checks the inference results. This is the end of the tutorial.\n",
        "\n",
        "If you encounter problems during training or testing, please check the [common troubleshooting steps](https://mmyolo.readthedocs.io/en/dev/recommended_topics/troubleshooting_steps.html) first and feel free to open an [issue](https://github.com/open-mmlab/mmyolo/issues/new/choose) if you still can't solve it.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
